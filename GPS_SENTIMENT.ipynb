{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083e2fbd-f5ba-464e-a749-e72ec9a235d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195001e-c4d7-44de-8f82-8a15c0a3d46b",
   "metadata": {},
   "source": [
    "This project is based on Google play store data available in https://www.kaggle.com/datasets/lava18/google-play-store-apps, 10k Play Store apps for analysing the Android market.\n",
    "\n",
    "After reviews processing, we'll train 2 deep learning models with diffrent architectures.\n",
    "\n",
    "**For the statistical study please check GooglePlayStore_Analysis study on my Github**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d9d955-afa9-4db0-8af7-a264d08189af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64295, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gps_rev = pd.read_csv('googleplaystore_user_reviews.csv')\n",
    "gps_rev.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c7ff2f-2e54-459f-923e-50b6523f41d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "      <th>Sentiment_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>I like eat delicious food. That's I'm cooking ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>This help eating healthy exercise regular basis</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.288462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Works great especially going grocery store</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best idea us</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     App                                  Translated_Review  \\\n",
       "0  10 Best Foods for You  I like eat delicious food. That's I'm cooking ...   \n",
       "1  10 Best Foods for You    This help eating healthy exercise regular basis   \n",
       "2  10 Best Foods for You                                                NaN   \n",
       "3  10 Best Foods for You         Works great especially going grocery store   \n",
       "4  10 Best Foods for You                                       Best idea us   \n",
       "\n",
       "  Sentiment  Sentiment_Polarity  Sentiment_Subjectivity  \n",
       "0  Positive                1.00                0.533333  \n",
       "1  Positive                0.25                0.288462  \n",
       "2       NaN                 NaN                     NaN  \n",
       "3  Positive                0.40                0.875000  \n",
       "4  Positive                1.00                0.300000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gps_rev.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a78aeb0-99b7-43a9-a4aa-8861cfff888c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I like eat delicious food. That's I'm cooking ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This help eating healthy exercise regular basis</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Works great especially going grocery store</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Best idea us</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64290</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64291</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64292</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64293</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64294</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64295 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Translated_Review Sentiment\n",
       "0      I like eat delicious food. That's I'm cooking ...  Positive\n",
       "1        This help eating healthy exercise regular basis  Positive\n",
       "2                                                    NaN       NaN\n",
       "3             Works great especially going grocery store  Positive\n",
       "4                                           Best idea us  Positive\n",
       "...                                                  ...       ...\n",
       "64290                                                NaN       NaN\n",
       "64291                                                NaN       NaN\n",
       "64292                                                NaN       NaN\n",
       "64293                                                NaN       NaN\n",
       "64294                                                NaN       NaN\n",
       "\n",
       "[64295 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gps_rev[['Translated_Review', 'Sentiment']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18f84989-4452-402c-a2fa-cad94507061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_rev = gps_rev.dropna()\n",
    "gps_rev = gps_rev.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d90098-2370-4451-a9c3-a665edc371f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uniques values\n",
    "gps_rev['Sentiment'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a583a56-3b2c-4f05-a7ff-e1abed9fbb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Null VALUES\n",
    "gps_rev[\"Translated_Review\"].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cbb99bd-f43e-49f3-94cc-6d85e2dc544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depure_data(data):\n",
    "    \n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(\"\\'\" , \"\", data)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c0fedd2-d567-4acb-9f1f-b8641ad5d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rev = []\n",
    "#Splitting gps_rev to list\n",
    "data_to_list = gps_rev[\"Translated_Review\"].values.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ece5915-1405-4295-9000-cb62bbac02d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I like eat delicious food. Thats Im cooking food myself, case \"10 Best Foods\" helps lot, also \"Best Before (Shelf Life)\"',\n",
       " 'This help eating healthy exercise regular basis',\n",
       " 'Works great especially going grocery store',\n",
       " 'Best idea us',\n",
       " 'Best way']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(data_to_list)):\n",
    "    list_rev.append(depure_data(data_to_list[i]))\n",
    "list(list_rev[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d24122cb-5912-444e-b205-97db596faadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to understand yield https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "#Break sentences into words:\n",
    "def break_sent2word(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "        #gensim... Convert sentences into a list of tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31f1c325-de25-4fec-9424-8d8d4d128335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['like', 'eat', 'delicious', 'food', 'thats', 'im', 'cooking', 'food', 'myself', 'case', 'best', 'foods', 'helps', 'lot', 'also', 'best', 'before', 'shelf', 'life'], ['this', 'help', 'eating', 'healthy', 'exercise', 'regular', 'basis'], ['works', 'great', 'especially', 'going', 'grocery', 'store'], ['best', 'idea', 'us'], ['best', 'way'], ['amazing'], ['looking', 'forward', 'app'], ['it', 'helpful', 'site', 'it', 'help', 'foods', 'get'], ['good', 'you'], ['useful', 'information', 'the', 'amount', 'spelling', 'errors', 'questions', 'validity', 'information', 'shared', 'once', 'fixed', 'stars', 'given'], ['thank', 'you', 'great', 'app', 'add', 'arthritis', 'eyes', 'immunity', 'kidney', 'liver', 'detox', 'foods', 'please'], ['greatest', 'ever', 'completely', 'awesome', 'maintain', 'health', 'this', 'must', 'ppl', 'there', 'love', 'it'], ['good', 'health', 'good', 'health', 'first', 'priority'], ['health', 'its', 'important', 'world', 'either', 'life', 'think'], ['mrs', 'sunita', 'bhati', 'thankful', 'developers', 'to', 'make', 'kind', 'app', 'really', 'good', 'healthy', 'food', 'body'], ['very', 'useful', 'in', 'diabetes', 'age', 'need', 'control', 'sugar', 'thanks'], ['one', 'greatest', 'apps'], ['good', 'nice'], ['healthy', 'really', 'helped'], ['god', 'health']]\n"
     ]
    }
   ],
   "source": [
    "data_words = list(break_sent2word(list_rev))\n",
    "\n",
    "print(data_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a36a4c70-a040-49be-b416-2527d21a2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(text):\n",
    "    return TreebankWordDetokenizer().detokenize(text)\n",
    "\n",
    "# to understand about TreebankWordDetokenizer\n",
    "#https://www.nltk.org/_modules/nltk/tokenize/treebank.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27cfa0fa-fef5-41ce-84b1-06204916ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_rev = []\n",
    "for i in range(len(data_words)):\n",
    "    treated_rev.append(detokenize(data_words[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d65308b4-6848-4225-8a5c-2dd1fcd7a035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like eat delicious food thats im cooking food myself case best foods helps lot also best before shelf life', 'this help eating healthy exercise regular basis', 'works great especially going grocery store', 'best idea us', 'best way', 'amazing', 'looking forward app', 'it helpful site it help foods get', 'good you', 'useful information the amount spelling errors questions validity information shared once fixed stars given', 'thank you great app add arthritis eyes immunity kidney liver detox foods please', 'greatest ever completely awesome maintain health this must ppl there love it', 'good health good health first priority', 'health its important world either life think', 'mrs sunita bhati thankful developers to make kind app really good healthy food body', 'very useful in diabetes age need control sugar thanks', 'one greatest apps', 'good nice', 'healthy really helped', 'god health']\n"
     ]
    }
   ],
   "source": [
    "print(treated_rev[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dfa30c6-e72d-40b7-9412-09d52c338e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_rev = np.array(treated_rev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7300a5cb-e834-4ea2-8a5c-17ae661bac64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['like eat delicious food thats im cooking food myself case best foods helps lot also best before shelf life',\n",
       "       'this help eating healthy exercise regular basis',\n",
       "       'works great especially going grocery store', ...,\n",
       "       'dumb app wanted post property rent give option website work waste time space phone',\n",
       "       'property business got link sms happy performance still guys need raise bar guys cheers',\n",
       "       'useless app searched flats kondapur hyderabad none number reachable know flats unavailable would keep posts active'],\n",
       "      dtype='<U2513')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treated_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b05f4-68ba-4b44-ac18-b7db75d0ffcf",
   "metadata": {},
   "source": [
    "## Creating new column Sentiment_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e2f66-75e0-46ab-80c1-1dc71d2166ec",
   "metadata": {},
   "source": [
    "convert the Sentiment labels from (Neutral, Negative, Positive) to a float type that our model can understand. To achieve this task, we'll implement the **to_categorical method** from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31a7fb5b-41fc-4fa9-9c7f-2bf75374d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_labeled = np.array(gps_rev['Sentiment'])\n",
    "y = []\n",
    "for i in range(len(sent_labeled)):\n",
    "    if sent_labeled[i] == 'Neutral':\n",
    "        y.append(0)\n",
    "    if sent_labeled[i] == 'Negative':\n",
    "        y.append(1)\n",
    "    if sent_labeled[i] == 'Positive':\n",
    "        y.append(2)\n",
    "y = np.array(y)\n",
    "sent_labeled = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b4fa6-4723-44d6-af8f-23cf1a3cdd24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b71f1ac1-066e-4718-94cb-f7ab4341e5bc",
   "metadata": {},
   "source": [
    "We'll implement the Keras tokenizer as well as its pad_sequences method to transform our text data into 3D float data, otherwise our neural networks won't be able to be trained on it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ffb8722-6638-4ca8-9d6d-178071d10bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop,Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72e99e9c-b274-46f0-95b8-04cd6da1b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fc60035-c97a-4b7b-a0b0-0c1ba4d8c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "max_words = 5000\n",
    "max_len = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e4017-5a1c-4c0f-b363-0f90fa37708c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "keras.preprocessing.text:\n",
    "This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n",
    "\n",
    "By default, all punctuation is removed, turning the texts into space-separated sequences of words (words maybe include the ' character). These sequences are then split into lists of tokens. They will then be indexed or vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ed90856-a86e-4261-a79c-bdc842cab4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(treated_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44e477-a452-411d-824f-6386281f7a7f",
   "metadata": {},
   "source": [
    "**pad_sequences** is used to ensure that all sequences in a list have the same length. By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence.\n",
    "\n",
    "**texts_to_sequences** method helps in converting tokens of text corpus into a sequence of integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "192db438-5eb4-4797-962f-ed04d3d820f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  500 4799  266]\n",
      " [   0    0    0 ... 1042 1026 2149]\n",
      " [   0    0    0 ...  127 1874  323]\n",
      " ...\n",
      " [   0    0    0 ...   11  504   22]\n",
      " [   0    0    0 ...  605  219 3154]\n",
      " [   0    0    0 ...   58 1281 1206]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(treated_rev)\n",
    "rev_vect_padded = pad_sequences(sequences, maxlen=max_len)\n",
    "print(rev_vect_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c238aa66-55da-4f72-a33e-aa53c47d17b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sent_labeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee2389e4-fda0-4987-b896-3a40b317ca1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28070 9357 28070 9357\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(rev_vect_padded,sent_labeled, random_state=0)\n",
    "print (len(X_train),len(X_test),len(y_train),len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21342d91-42e1-4752-b409-524fdc290b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "142b6ed2-5ebd-46bb-a472-b2d48a462787",
   "metadata": {},
   "source": [
    "## Model building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b3831-3119-4e65-b6f5-dfb7705eca46",
   "metadata": {},
   "source": [
    "Embedding layer enables us to convert each word into a fixed length vector of defined size. The resultant vector is a dense one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6158bd71-d4c1-4980-95af-391e826b1cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSingle LSTM layer model\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Single LSTM layer model\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f9af5d9-4e46-4e0a-84d8-82e0eb580b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 11:40:52.341029: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.6409 - accuracy: 0.7266\n",
      "Epoch 1: val_accuracy improved from -inf to 0.81714, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 55s 61ms/step - loss: 0.6409 - accuracy: 0.7266 - val_loss: 0.4635 - val_accuracy: 0.8171\n",
      "Epoch 2/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.4027 - accuracy: 0.8439\n",
      "Epoch 2: val_accuracy improved from 0.81714 to 0.87624, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 51s 58ms/step - loss: 0.4027 - accuracy: 0.8439 - val_loss: 0.3482 - val_accuracy: 0.8762\n",
      "Epoch 3/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.8808\n",
      "Epoch 3: val_accuracy improved from 0.87624 to 0.88469, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 50s 57ms/step - loss: 0.3291 - accuracy: 0.8808 - val_loss: 0.3288 - val_accuracy: 0.8847\n",
      "Epoch 4/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.8962\n",
      "Epoch 4: val_accuracy improved from 0.88469 to 0.90520, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 53s 60ms/step - loss: 0.2930 - accuracy: 0.8962 - val_loss: 0.2806 - val_accuracy: 0.9052\n",
      "Epoch 5/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.9027\n",
      "Epoch 5: val_accuracy improved from 0.90520 to 0.91258, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 51s 58ms/step - loss: 0.2730 - accuracy: 0.9027 - val_loss: 0.2657 - val_accuracy: 0.9126\n",
      "Epoch 6/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.9116\n",
      "Epoch 6: val_accuracy did not improve from 0.91258\n",
      "878/878 [==============================] - 56s 64ms/step - loss: 0.2537 - accuracy: 0.9116 - val_loss: 0.2616 - val_accuracy: 0.9114\n",
      "Epoch 7/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.9160\n",
      "Epoch 7: val_accuracy did not improve from 0.91258\n",
      "878/878 [==============================] - 52s 59ms/step - loss: 0.2409 - accuracy: 0.9160 - val_loss: 0.2561 - val_accuracy: 0.9108\n",
      "Epoch 8/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2336 - accuracy: 0.9192\n",
      "Epoch 8: val_accuracy improved from 0.91258 to 0.91482, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 51s 58ms/step - loss: 0.2336 - accuracy: 0.9192 - val_loss: 0.2623 - val_accuracy: 0.9148\n",
      "Epoch 9/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2263 - accuracy: 0.9221\n",
      "Epoch 9: val_accuracy did not improve from 0.91482\n",
      "878/878 [==============================] - 51s 58ms/step - loss: 0.2263 - accuracy: 0.9221 - val_loss: 0.2594 - val_accuracy: 0.9133\n",
      "Epoch 10/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2237 - accuracy: 0.9230\n",
      "Epoch 10: val_accuracy improved from 0.91482 to 0.91888, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 54s 62ms/step - loss: 0.2237 - accuracy: 0.9230 - val_loss: 0.2475 - val_accuracy: 0.9189\n",
      "Epoch 11/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9254\n",
      "Epoch 11: val_accuracy improved from 0.91888 to 0.92198, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 54s 62ms/step - loss: 0.2176 - accuracy: 0.9254 - val_loss: 0.2435 - val_accuracy: 0.9220\n",
      "Epoch 12/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2166 - accuracy: 0.9278\n",
      "Epoch 12: val_accuracy did not improve from 0.92198\n",
      "878/878 [==============================] - 52s 59ms/step - loss: 0.2166 - accuracy: 0.9278 - val_loss: 0.2582 - val_accuracy: 0.9141\n",
      "Epoch 13/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2112 - accuracy: 0.9279\n",
      "Epoch 13: val_accuracy did not improve from 0.92198\n",
      "878/878 [==============================] - 53s 61ms/step - loss: 0.2112 - accuracy: 0.9279 - val_loss: 0.2434 - val_accuracy: 0.9205\n",
      "Epoch 14/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.9292\n",
      "Epoch 14: val_accuracy did not improve from 0.92198\n",
      "878/878 [==============================] - 52s 59ms/step - loss: 0.2086 - accuracy: 0.9292 - val_loss: 0.2467 - val_accuracy: 0.9212\n",
      "Epoch 15/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.9296\n",
      "Epoch 15: val_accuracy improved from 0.92198 to 0.92220, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 51s 59ms/step - loss: 0.2047 - accuracy: 0.9296 - val_loss: 0.2407 - val_accuracy: 0.9222\n",
      "Epoch 16/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.9317\n",
      "Epoch 16: val_accuracy did not improve from 0.92220\n",
      "878/878 [==============================] - 52s 59ms/step - loss: 0.2015 - accuracy: 0.9317 - val_loss: 0.2357 - val_accuracy: 0.9194\n",
      "Epoch 17/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2004 - accuracy: 0.9320\n",
      "Epoch 17: val_accuracy did not improve from 0.92220\n",
      "878/878 [==============================] - 53s 60ms/step - loss: 0.2004 - accuracy: 0.9320 - val_loss: 0.2459 - val_accuracy: 0.9209\n",
      "Epoch 18/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1992 - accuracy: 0.9320\n",
      "Epoch 18: val_accuracy improved from 0.92220 to 0.92369, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 52s 59ms/step - loss: 0.1992 - accuracy: 0.9320 - val_loss: 0.2375 - val_accuracy: 0.9237\n",
      "Epoch 19/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9349\n",
      "Epoch 19: val_accuracy improved from 0.92369 to 0.92615, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 54s 62ms/step - loss: 0.1932 - accuracy: 0.9349 - val_loss: 0.2316 - val_accuracy: 0.9262\n",
      "Epoch 20/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1956 - accuracy: 0.9335\n",
      "Epoch 20: val_accuracy did not improve from 0.92615\n",
      "878/878 [==============================] - 54s 61ms/step - loss: 0.1956 - accuracy: 0.9335 - val_loss: 0.2416 - val_accuracy: 0.9216\n",
      "Epoch 21/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1908 - accuracy: 0.9340\n",
      "Epoch 21: val_accuracy did not improve from 0.92615\n",
      "878/878 [==============================] - 54s 62ms/step - loss: 0.1908 - accuracy: 0.9340 - val_loss: 0.2332 - val_accuracy: 0.9207\n",
      "Epoch 22/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1878 - accuracy: 0.9368\n",
      "Epoch 22: val_accuracy did not improve from 0.92615\n",
      "878/878 [==============================] - 54s 62ms/step - loss: 0.1878 - accuracy: 0.9368 - val_loss: 0.2271 - val_accuracy: 0.9233\n",
      "Epoch 23/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1910 - accuracy: 0.9350\n",
      "Epoch 23: val_accuracy did not improve from 0.92615\n",
      "878/878 [==============================] - 54s 61ms/step - loss: 0.1910 - accuracy: 0.9350 - val_loss: 0.2240 - val_accuracy: 0.9247\n",
      "Epoch 24/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9380\n",
      "Epoch 24: val_accuracy did not improve from 0.92615\n",
      "878/878 [==============================] - 52s 60ms/step - loss: 0.1856 - accuracy: 0.9380 - val_loss: 0.2417 - val_accuracy: 0.9243\n",
      "Epoch 25/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1853 - accuracy: 0.9382\n",
      "Epoch 25: val_accuracy did not improve from 0.92615\n",
      "878/878 [==============================] - 52s 59ms/step - loss: 0.1853 - accuracy: 0.9382 - val_loss: 0.2267 - val_accuracy: 0.9253\n",
      "Epoch 26/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1837 - accuracy: 0.9374\n",
      "Epoch 26: val_accuracy improved from 0.92615 to 0.92765, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 52s 59ms/step - loss: 0.1837 - accuracy: 0.9374 - val_loss: 0.2244 - val_accuracy: 0.9276\n",
      "Epoch 27/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1812 - accuracy: 0.9379\n",
      "Epoch 27: val_accuracy improved from 0.92765 to 0.92989, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 52s 60ms/step - loss: 0.1812 - accuracy: 0.9379 - val_loss: 0.2197 - val_accuracy: 0.9299\n",
      "Epoch 28/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9394\n",
      "Epoch 28: val_accuracy did not improve from 0.92989\n",
      "878/878 [==============================] - 52s 59ms/step - loss: 0.1774 - accuracy: 0.9394 - val_loss: 0.2241 - val_accuracy: 0.9259\n",
      "Epoch 29/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9403\n",
      "Epoch 29: val_accuracy did not improve from 0.92989\n",
      "878/878 [==============================] - 52s 59ms/step - loss: 0.1760 - accuracy: 0.9403 - val_loss: 0.2232 - val_accuracy: 0.9284\n",
      "Epoch 30/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1761 - accuracy: 0.9410\n",
      "Epoch 30: val_accuracy did not improve from 0.92989\n",
      "878/878 [==============================] - 52s 59ms/step - loss: 0.1761 - accuracy: 0.9410 - val_loss: 0.2259 - val_accuracy: 0.9275\n",
      "Epoch 31/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1743 - accuracy: 0.9413\n",
      "Epoch 31: val_accuracy did not improve from 0.92989\n",
      "878/878 [==============================] - 52s 60ms/step - loss: 0.1743 - accuracy: 0.9413 - val_loss: 0.2242 - val_accuracy: 0.9298\n",
      "Epoch 32/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9425\n",
      "Epoch 32: val_accuracy did not improve from 0.92989\n",
      "878/878 [==============================] - 53s 60ms/step - loss: 0.1723 - accuracy: 0.9425 - val_loss: 0.2281 - val_accuracy: 0.9278\n",
      "Epoch 33/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1748 - accuracy: 0.9424\n",
      "Epoch 33: val_accuracy did not improve from 0.92989\n",
      "878/878 [==============================] - 56s 64ms/step - loss: 0.1748 - accuracy: 0.9424 - val_loss: 0.2349 - val_accuracy: 0.9207\n",
      "Epoch 34/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9422\n",
      "Epoch 34: val_accuracy did not improve from 0.92989\n",
      "878/878 [==============================] - 54s 61ms/step - loss: 0.1720 - accuracy: 0.9422 - val_loss: 0.2282 - val_accuracy: 0.9260\n",
      "Epoch 35/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1671 - accuracy: 0.9432\n",
      "Epoch 35: val_accuracy improved from 0.92989 to 0.93043, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 56s 64ms/step - loss: 0.1671 - accuracy: 0.9432 - val_loss: 0.2214 - val_accuracy: 0.9304\n",
      "Epoch 36/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9424\n",
      "Epoch 36: val_accuracy did not improve from 0.93043\n",
      "878/878 [==============================] - 60s 68ms/step - loss: 0.1713 - accuracy: 0.9424 - val_loss: 0.2257 - val_accuracy: 0.9233\n",
      "Epoch 37/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9436\n",
      "Epoch 37: val_accuracy did not improve from 0.93043\n",
      "878/878 [==============================] - 54s 61ms/step - loss: 0.1672 - accuracy: 0.9436 - val_loss: 0.2238 - val_accuracy: 0.9240\n",
      "Epoch 38/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9437\n",
      "Epoch 38: val_accuracy did not improve from 0.93043\n",
      "878/878 [==============================] - 53s 61ms/step - loss: 0.1677 - accuracy: 0.9437 - val_loss: 0.2194 - val_accuracy: 0.9294\n",
      "Epoch 39/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9450\n",
      "Epoch 39: val_accuracy did not improve from 0.93043\n",
      "878/878 [==============================] - 52s 60ms/step - loss: 0.1641 - accuracy: 0.9450 - val_loss: 0.2204 - val_accuracy: 0.9296\n",
      "Epoch 40/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.9445\n",
      "Epoch 40: val_accuracy did not improve from 0.93043\n",
      "878/878 [==============================] - 54s 61ms/step - loss: 0.1639 - accuracy: 0.9445 - val_loss: 0.2265 - val_accuracy: 0.9226\n",
      "Epoch 41/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9460\n",
      "Epoch 41: val_accuracy did not improve from 0.93043\n",
      "878/878 [==============================] - 54s 62ms/step - loss: 0.1627 - accuracy: 0.9460 - val_loss: 0.2224 - val_accuracy: 0.9297\n",
      "Epoch 42/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9455\n",
      "Epoch 42: val_accuracy did not improve from 0.93043\n",
      "878/878 [==============================] - 53s 61ms/step - loss: 0.1586 - accuracy: 0.9455 - val_loss: 0.2278 - val_accuracy: 0.9288\n",
      "Epoch 43/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9451\n",
      "Epoch 43: val_accuracy did not improve from 0.93043\n",
      "878/878 [==============================] - 53s 60ms/step - loss: 0.1610 - accuracy: 0.9451 - val_loss: 0.2337 - val_accuracy: 0.9251\n",
      "Epoch 44/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.9450\n",
      "Epoch 44: val_accuracy did not improve from 0.93043\n",
      "878/878 [==============================] - 53s 60ms/step - loss: 0.1631 - accuracy: 0.9450 - val_loss: 0.2220 - val_accuracy: 0.9291\n",
      "Epoch 45/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.9467\n",
      "Epoch 45: val_accuracy did not improve from 0.93043\n",
      "878/878 [==============================] - 53s 61ms/step - loss: 0.1606 - accuracy: 0.9467 - val_loss: 0.2355 - val_accuracy: 0.9225\n",
      "Epoch 46/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9460\n",
      "Epoch 46: val_accuracy did not improve from 0.93043\n",
      "878/878 [==============================] - 55s 63ms/step - loss: 0.1581 - accuracy: 0.9460 - val_loss: 0.2319 - val_accuracy: 0.9249\n",
      "Epoch 47/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9480\n",
      "Epoch 47: val_accuracy improved from 0.93043 to 0.93267, saving model to best_model1.hdf5\n",
      "878/878 [==============================] - 53s 60ms/step - loss: 0.1571 - accuracy: 0.9480 - val_loss: 0.2201 - val_accuracy: 0.9327\n",
      "Epoch 48/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.9463\n",
      "Epoch 48: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 53s 60ms/step - loss: 0.1570 - accuracy: 0.9463 - val_loss: 0.2303 - val_accuracy: 0.9249\n",
      "Epoch 49/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9477\n",
      "Epoch 49: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 55s 63ms/step - loss: 0.1575 - accuracy: 0.9477 - val_loss: 0.2376 - val_accuracy: 0.9256\n",
      "Epoch 50/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9481\n",
      "Epoch 50: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 56s 64ms/step - loss: 0.1548 - accuracy: 0.9481 - val_loss: 0.2293 - val_accuracy: 0.9275\n",
      "Epoch 51/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1552 - accuracy: 0.9476\n",
      "Epoch 51: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 66s 75ms/step - loss: 0.1552 - accuracy: 0.9476 - val_loss: 0.2305 - val_accuracy: 0.9279\n",
      "Epoch 52/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.9483\n",
      "Epoch 52: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 55s 63ms/step - loss: 0.1565 - accuracy: 0.9483 - val_loss: 0.2365 - val_accuracy: 0.9242\n",
      "Epoch 53/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9469\n",
      "Epoch 53: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 56s 64ms/step - loss: 0.1579 - accuracy: 0.9469 - val_loss: 0.2322 - val_accuracy: 0.9237\n",
      "Epoch 54/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1561 - accuracy: 0.9471\n",
      "Epoch 54: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 55s 63ms/step - loss: 0.1561 - accuracy: 0.9471 - val_loss: 0.2321 - val_accuracy: 0.9262\n",
      "Epoch 55/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9478\n",
      "Epoch 55: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 57s 64ms/step - loss: 0.1554 - accuracy: 0.9478 - val_loss: 0.2315 - val_accuracy: 0.9247\n",
      "Epoch 56/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1507 - accuracy: 0.9493\n",
      "Epoch 56: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 57s 64ms/step - loss: 0.1507 - accuracy: 0.9493 - val_loss: 0.2376 - val_accuracy: 0.9239\n",
      "Epoch 57/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.9504\n",
      "Epoch 57: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 57s 65ms/step - loss: 0.1505 - accuracy: 0.9504 - val_loss: 0.2576 - val_accuracy: 0.9162\n",
      "Epoch 58/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9471\n",
      "Epoch 58: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 54s 62ms/step - loss: 0.1569 - accuracy: 0.9471 - val_loss: 0.2326 - val_accuracy: 0.9264\n",
      "Epoch 59/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.9505\n",
      "Epoch 59: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 55s 62ms/step - loss: 0.1538 - accuracy: 0.9505 - val_loss: 0.2522 - val_accuracy: 0.9151\n",
      "Epoch 60/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9474\n",
      "Epoch 60: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 57s 65ms/step - loss: 0.1572 - accuracy: 0.9474 - val_loss: 0.2241 - val_accuracy: 0.9253\n",
      "Epoch 61/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1523 - accuracy: 0.9488\n",
      "Epoch 61: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 55s 62ms/step - loss: 0.1523 - accuracy: 0.9488 - val_loss: 0.2353 - val_accuracy: 0.9239\n",
      "Epoch 62/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9510\n",
      "Epoch 62: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 55s 63ms/step - loss: 0.1508 - accuracy: 0.9510 - val_loss: 0.2297 - val_accuracy: 0.9269\n",
      "Epoch 63/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9519\n",
      "Epoch 63: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 56s 64ms/step - loss: 0.1484 - accuracy: 0.9519 - val_loss: 0.2290 - val_accuracy: 0.9266\n",
      "Epoch 64/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1514 - accuracy: 0.9502\n",
      "Epoch 64: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 54s 62ms/step - loss: 0.1514 - accuracy: 0.9502 - val_loss: 0.2669 - val_accuracy: 0.9140\n",
      "Epoch 65/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.9496\n",
      "Epoch 65: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 54s 62ms/step - loss: 0.1512 - accuracy: 0.9496 - val_loss: 0.2390 - val_accuracy: 0.9251\n",
      "Epoch 66/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1488 - accuracy: 0.9501\n",
      "Epoch 66: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 53s 60ms/step - loss: 0.1488 - accuracy: 0.9501 - val_loss: 0.2335 - val_accuracy: 0.9234\n",
      "Epoch 67/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.9511\n",
      "Epoch 67: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 53s 60ms/step - loss: 0.1502 - accuracy: 0.9511 - val_loss: 0.2337 - val_accuracy: 0.9223\n",
      "Epoch 68/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.9503\n",
      "Epoch 68: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 53s 60ms/step - loss: 0.1499 - accuracy: 0.9503 - val_loss: 0.2390 - val_accuracy: 0.9235\n",
      "Epoch 69/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.9524\n",
      "Epoch 69: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 53s 60ms/step - loss: 0.1467 - accuracy: 0.9524 - val_loss: 0.2464 - val_accuracy: 0.9191\n",
      "Epoch 70/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9517\n",
      "Epoch 70: val_accuracy did not improve from 0.93267\n",
      "878/878 [==============================] - 53s 60ms/step - loss: 0.1481 - accuracy: 0.9517 - val_loss: 0.2397 - val_accuracy: 0.9238\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(layers.Embedding(max_words, 20))\n",
    "model1.add(layers.LSTM(15,dropout=0.5))\n",
    "model1.add(layers.Dense(3,activation='softmax'))\n",
    "\n",
    "\n",
    "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model1.fit(X_train, y_train, epochs=70,validation_data=(X_test, y_test),callbacks=[checkpoint1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11975591-ce69-450a-87a3-8288d05818a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5be251f-3acc-40d3-a8ab-1aa9f27e5fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBidirectional LTSM model\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bidirectional LTSM model\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "603077a8-97a0-4539-9e1b-2157147893ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.6196 - accuracy: 0.7293\n",
      "Epoch 1: val_accuracy improved from -inf to 0.82420, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 81s 89ms/step - loss: 0.6196 - accuracy: 0.7293 - val_loss: 0.4507 - val_accuracy: 0.8242\n",
      "Epoch 2/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.3882 - accuracy: 0.8526\n",
      "Epoch 2: val_accuracy improved from 0.82420 to 0.87977, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 77s 88ms/step - loss: 0.3882 - accuracy: 0.8526 - val_loss: 0.3425 - val_accuracy: 0.8798\n",
      "Epoch 3/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.3199 - accuracy: 0.8846\n",
      "Epoch 3: val_accuracy improved from 0.87977 to 0.89046, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 78s 89ms/step - loss: 0.3199 - accuracy: 0.8846 - val_loss: 0.3070 - val_accuracy: 0.8905\n",
      "Epoch 4/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.8964\n",
      "Epoch 4: val_accuracy improved from 0.89046 to 0.90435, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 79s 90ms/step - loss: 0.2874 - accuracy: 0.8964 - val_loss: 0.2717 - val_accuracy: 0.9043\n",
      "Epoch 5/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.9076\n",
      "Epoch 5: val_accuracy improved from 0.90435 to 0.91119, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 78s 89ms/step - loss: 0.2630 - accuracy: 0.9076 - val_loss: 0.2681 - val_accuracy: 0.9112\n",
      "Epoch 6/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.9141\n",
      "Epoch 6: val_accuracy did not improve from 0.91119\n",
      "878/878 [==============================] - 78s 89ms/step - loss: 0.2481 - accuracy: 0.9141 - val_loss: 0.2792 - val_accuracy: 0.9084\n",
      "Epoch 7/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2386 - accuracy: 0.9179\n",
      "Epoch 7: val_accuracy did not improve from 0.91119\n",
      "878/878 [==============================] - 80s 91ms/step - loss: 0.2386 - accuracy: 0.9179 - val_loss: 0.2959 - val_accuracy: 0.9076\n",
      "Epoch 8/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2307 - accuracy: 0.9216\n",
      "Epoch 8: val_accuracy improved from 0.91119 to 0.91696, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 78s 89ms/step - loss: 0.2307 - accuracy: 0.9216 - val_loss: 0.2454 - val_accuracy: 0.9170\n",
      "Epoch 9/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9227\n",
      "Epoch 9: val_accuracy improved from 0.91696 to 0.92166, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 78s 89ms/step - loss: 0.2260 - accuracy: 0.9227 - val_loss: 0.2424 - val_accuracy: 0.9217\n",
      "Epoch 10/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.9260\n",
      "Epoch 10: val_accuracy improved from 0.92166 to 0.92295, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 78s 89ms/step - loss: 0.2217 - accuracy: 0.9260 - val_loss: 0.2455 - val_accuracy: 0.9229\n",
      "Epoch 11/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2179 - accuracy: 0.9255\n",
      "Epoch 11: val_accuracy did not improve from 0.92295\n",
      "878/878 [==============================] - 78s 89ms/step - loss: 0.2179 - accuracy: 0.9255 - val_loss: 0.2496 - val_accuracy: 0.9178\n",
      "Epoch 12/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 0.9285\n",
      "Epoch 12: val_accuracy did not improve from 0.92295\n",
      "878/878 [==============================] - 78s 89ms/step - loss: 0.2103 - accuracy: 0.9285 - val_loss: 0.2383 - val_accuracy: 0.9220\n",
      "Epoch 13/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2073 - accuracy: 0.9296\n",
      "Epoch 13: val_accuracy did not improve from 0.92295\n",
      "878/878 [==============================] - 78s 89ms/step - loss: 0.2073 - accuracy: 0.9296 - val_loss: 0.2393 - val_accuracy: 0.9211\n",
      "Epoch 14/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.2016 - accuracy: 0.9310\n",
      "Epoch 14: val_accuracy did not improve from 0.92295\n",
      "878/878 [==============================] - 78s 88ms/step - loss: 0.2016 - accuracy: 0.9310 - val_loss: 0.2348 - val_accuracy: 0.9224\n",
      "Epoch 15/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9322\n",
      "Epoch 15: val_accuracy improved from 0.92295 to 0.92380, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 79s 90ms/step - loss: 0.1986 - accuracy: 0.9322 - val_loss: 0.2371 - val_accuracy: 0.9238\n",
      "Epoch 16/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9330\n",
      "Epoch 16: val_accuracy improved from 0.92380 to 0.92679, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 277s 315ms/step - loss: 0.1961 - accuracy: 0.9330 - val_loss: 0.2323 - val_accuracy: 0.9268\n",
      "Epoch 17/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1947 - accuracy: 0.9351\n",
      "Epoch 17: val_accuracy did not improve from 0.92679\n",
      "878/878 [==============================] - 81s 92ms/step - loss: 0.1947 - accuracy: 0.9351 - val_loss: 0.2382 - val_accuracy: 0.9268\n",
      "Epoch 18/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1902 - accuracy: 0.9357\n",
      "Epoch 18: val_accuracy did not improve from 0.92679\n",
      "878/878 [==============================] - 79s 90ms/step - loss: 0.1902 - accuracy: 0.9357 - val_loss: 0.2277 - val_accuracy: 0.9264\n",
      "Epoch 19/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.9362\n",
      "Epoch 19: val_accuracy improved from 0.92679 to 0.92690, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 81s 92ms/step - loss: 0.1904 - accuracy: 0.9362 - val_loss: 0.2241 - val_accuracy: 0.9269\n",
      "Epoch 20/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1861 - accuracy: 0.9365\n",
      "Epoch 20: val_accuracy did not improve from 0.92690\n",
      "878/878 [==============================] - 79s 90ms/step - loss: 0.1861 - accuracy: 0.9365 - val_loss: 0.2346 - val_accuracy: 0.9252\n",
      "Epoch 21/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1836 - accuracy: 0.9388\n",
      "Epoch 21: val_accuracy did not improve from 0.92690\n",
      "878/878 [==============================] - 79s 90ms/step - loss: 0.1836 - accuracy: 0.9388 - val_loss: 0.2292 - val_accuracy: 0.9244\n",
      "Epoch 22/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1816 - accuracy: 0.9403\n",
      "Epoch 22: val_accuracy improved from 0.92690 to 0.92861, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 82s 93ms/step - loss: 0.1816 - accuracy: 0.9403 - val_loss: 0.2218 - val_accuracy: 0.9286\n",
      "Epoch 23/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1812 - accuracy: 0.9394\n",
      "Epoch 23: val_accuracy did not improve from 0.92861\n",
      "878/878 [==============================] - 80s 91ms/step - loss: 0.1812 - accuracy: 0.9394 - val_loss: 0.2430 - val_accuracy: 0.9253\n",
      "Epoch 24/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1795 - accuracy: 0.9388\n",
      "Epoch 24: val_accuracy improved from 0.92861 to 0.92957, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 80s 91ms/step - loss: 0.1795 - accuracy: 0.9388 - val_loss: 0.2176 - val_accuracy: 0.9296\n",
      "Epoch 25/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9414\n",
      "Epoch 25: val_accuracy improved from 0.92957 to 0.93096, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 79s 90ms/step - loss: 0.1759 - accuracy: 0.9414 - val_loss: 0.2224 - val_accuracy: 0.9310\n",
      "Epoch 26/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1732 - accuracy: 0.9428\n",
      "Epoch 26: val_accuracy did not improve from 0.93096\n",
      "878/878 [==============================] - 80s 91ms/step - loss: 0.1732 - accuracy: 0.9428 - val_loss: 0.2276 - val_accuracy: 0.9283\n",
      "Epoch 27/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9426\n",
      "Epoch 27: val_accuracy did not improve from 0.93096\n",
      "878/878 [==============================] - 82s 93ms/step - loss: 0.1707 - accuracy: 0.9426 - val_loss: 0.2358 - val_accuracy: 0.9249\n",
      "Epoch 28/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1730 - accuracy: 0.9420\n",
      "Epoch 28: val_accuracy improved from 0.93096 to 0.93224, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 81s 92ms/step - loss: 0.1730 - accuracy: 0.9420 - val_loss: 0.2159 - val_accuracy: 0.9322\n",
      "Epoch 29/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 0.9435\n",
      "Epoch 29: val_accuracy improved from 0.93224 to 0.93331, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 82s 94ms/step - loss: 0.1688 - accuracy: 0.9435 - val_loss: 0.2108 - val_accuracy: 0.9333\n",
      "Epoch 30/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9444\n",
      "Epoch 30: val_accuracy did not improve from 0.93331\n",
      "878/878 [==============================] - 83s 95ms/step - loss: 0.1677 - accuracy: 0.9444 - val_loss: 0.2104 - val_accuracy: 0.9313\n",
      "Epoch 31/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9456\n",
      "Epoch 31: val_accuracy did not improve from 0.93331\n",
      "878/878 [==============================] - 83s 95ms/step - loss: 0.1633 - accuracy: 0.9456 - val_loss: 0.2206 - val_accuracy: 0.9266\n",
      "Epoch 32/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9439\n",
      "Epoch 32: val_accuracy improved from 0.93331 to 0.93385, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 83s 95ms/step - loss: 0.1678 - accuracy: 0.9439 - val_loss: 0.2121 - val_accuracy: 0.9338\n",
      "Epoch 33/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.9467\n",
      "Epoch 33: val_accuracy did not improve from 0.93385\n",
      "878/878 [==============================] - 84s 96ms/step - loss: 0.1639 - accuracy: 0.9467 - val_loss: 0.2137 - val_accuracy: 0.9335\n",
      "Epoch 34/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9461\n",
      "Epoch 34: val_accuracy did not improve from 0.93385\n",
      "878/878 [==============================] - 83s 95ms/step - loss: 0.1633 - accuracy: 0.9461 - val_loss: 0.2214 - val_accuracy: 0.9287\n",
      "Epoch 35/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9458\n",
      "Epoch 35: val_accuracy improved from 0.93385 to 0.93427, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 81s 92ms/step - loss: 0.1616 - accuracy: 0.9458 - val_loss: 0.2127 - val_accuracy: 0.9343\n",
      "Epoch 36/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9481\n",
      "Epoch 36: val_accuracy did not improve from 0.93427\n",
      "878/878 [==============================] - 83s 95ms/step - loss: 0.1598 - accuracy: 0.9481 - val_loss: 0.2199 - val_accuracy: 0.9322\n",
      "Epoch 37/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9485\n",
      "Epoch 37: val_accuracy did not improve from 0.93427\n",
      "878/878 [==============================] - 83s 95ms/step - loss: 0.1572 - accuracy: 0.9485 - val_loss: 0.2091 - val_accuracy: 0.9319\n",
      "Epoch 38/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9481\n",
      "Epoch 38: val_accuracy did not improve from 0.93427\n",
      "878/878 [==============================] - 82s 93ms/step - loss: 0.1553 - accuracy: 0.9481 - val_loss: 0.2148 - val_accuracy: 0.9295\n",
      "Epoch 39/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1552 - accuracy: 0.9490\n",
      "Epoch 39: val_accuracy did not improve from 0.93427\n",
      "878/878 [==============================] - 82s 94ms/step - loss: 0.1552 - accuracy: 0.9490 - val_loss: 0.2133 - val_accuracy: 0.9328\n",
      "Epoch 40/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1560 - accuracy: 0.9481\n",
      "Epoch 40: val_accuracy did not improve from 0.93427\n",
      "878/878 [==============================] - 94s 107ms/step - loss: 0.1560 - accuracy: 0.9481 - val_loss: 0.2087 - val_accuracy: 0.9337\n",
      "Epoch 41/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9482\n",
      "Epoch 41: val_accuracy improved from 0.93427 to 0.93513, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 95s 108ms/step - loss: 0.1540 - accuracy: 0.9482 - val_loss: 0.2091 - val_accuracy: 0.9351\n",
      "Epoch 42/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1552 - accuracy: 0.9486\n",
      "Epoch 42: val_accuracy did not improve from 0.93513\n",
      "878/878 [==============================] - 94s 107ms/step - loss: 0.1552 - accuracy: 0.9486 - val_loss: 0.2231 - val_accuracy: 0.9349\n",
      "Epoch 43/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1514 - accuracy: 0.9500\n",
      "Epoch 43: val_accuracy did not improve from 0.93513\n",
      "878/878 [==============================] - 90s 103ms/step - loss: 0.1514 - accuracy: 0.9500 - val_loss: 0.2125 - val_accuracy: 0.9330\n",
      "Epoch 44/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1552 - accuracy: 0.9483\n",
      "Epoch 44: val_accuracy did not improve from 0.93513\n",
      "878/878 [==============================] - 87s 99ms/step - loss: 0.1552 - accuracy: 0.9483 - val_loss: 0.2159 - val_accuracy: 0.9301\n",
      "Epoch 45/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9497\n",
      "Epoch 45: val_accuracy did not improve from 0.93513\n",
      "878/878 [==============================] - 83s 94ms/step - loss: 0.1515 - accuracy: 0.9497 - val_loss: 0.2085 - val_accuracy: 0.9351\n",
      "Epoch 46/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1475 - accuracy: 0.9509\n",
      "Epoch 46: val_accuracy improved from 0.93513 to 0.93695, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 86s 98ms/step - loss: 0.1475 - accuracy: 0.9509 - val_loss: 0.2109 - val_accuracy: 0.9369\n",
      "Epoch 47/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1498 - accuracy: 0.9502\n",
      "Epoch 47: val_accuracy did not improve from 0.93695\n",
      "878/878 [==============================] - 84s 96ms/step - loss: 0.1498 - accuracy: 0.9502 - val_loss: 0.2151 - val_accuracy: 0.9343\n",
      "Epoch 48/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1466 - accuracy: 0.9515\n",
      "Epoch 48: val_accuracy did not improve from 0.93695\n",
      "878/878 [==============================] - 83s 94ms/step - loss: 0.1466 - accuracy: 0.9515 - val_loss: 0.2056 - val_accuracy: 0.9341\n",
      "Epoch 49/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1457 - accuracy: 0.9519\n",
      "Epoch 49: val_accuracy did not improve from 0.93695\n",
      "878/878 [==============================] - 83s 95ms/step - loss: 0.1457 - accuracy: 0.9519 - val_loss: 0.2166 - val_accuracy: 0.9294\n",
      "Epoch 50/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9515\n",
      "Epoch 50: val_accuracy did not improve from 0.93695\n",
      "878/878 [==============================] - 83s 94ms/step - loss: 0.1482 - accuracy: 0.9515 - val_loss: 0.2081 - val_accuracy: 0.9364\n",
      "Epoch 51/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9511\n",
      "Epoch 51: val_accuracy did not improve from 0.93695\n",
      "878/878 [==============================] - 81s 93ms/step - loss: 0.1483 - accuracy: 0.9511 - val_loss: 0.2233 - val_accuracy: 0.9330\n",
      "Epoch 52/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9521\n",
      "Epoch 52: val_accuracy did not improve from 0.93695\n",
      "878/878 [==============================] - 83s 95ms/step - loss: 0.1461 - accuracy: 0.9521 - val_loss: 0.2144 - val_accuracy: 0.9367\n",
      "Epoch 53/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1448 - accuracy: 0.9525\n",
      "Epoch 53: val_accuracy did not improve from 0.93695\n",
      "878/878 [==============================] - 82s 94ms/step - loss: 0.1448 - accuracy: 0.9525 - val_loss: 0.2180 - val_accuracy: 0.9326\n",
      "Epoch 54/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.9537\n",
      "Epoch 54: val_accuracy did not improve from 0.93695\n",
      "878/878 [==============================] - 82s 93ms/step - loss: 0.1432 - accuracy: 0.9537 - val_loss: 0.2126 - val_accuracy: 0.9362\n",
      "Epoch 55/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1438 - accuracy: 0.9523\n",
      "Epoch 55: val_accuracy did not improve from 0.93695\n",
      "878/878 [==============================] - 82s 93ms/step - loss: 0.1438 - accuracy: 0.9523 - val_loss: 0.2169 - val_accuracy: 0.9318\n",
      "Epoch 56/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1448 - accuracy: 0.9535\n",
      "Epoch 56: val_accuracy did not improve from 0.93695\n",
      "878/878 [==============================] - 83s 95ms/step - loss: 0.1448 - accuracy: 0.9535 - val_loss: 0.2147 - val_accuracy: 0.9345\n",
      "Epoch 57/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1436 - accuracy: 0.9519\n",
      "Epoch 57: val_accuracy improved from 0.93695 to 0.93801, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 83s 94ms/step - loss: 0.1436 - accuracy: 0.9519 - val_loss: 0.2077 - val_accuracy: 0.9380\n",
      "Epoch 58/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.9538\n",
      "Epoch 58: val_accuracy did not improve from 0.93801\n",
      "878/878 [==============================] - 83s 95ms/step - loss: 0.1422 - accuracy: 0.9538 - val_loss: 0.2073 - val_accuracy: 0.9378\n",
      "Epoch 59/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.9511\n",
      "Epoch 59: val_accuracy did not improve from 0.93801\n",
      "878/878 [==============================] - 81s 92ms/step - loss: 0.1431 - accuracy: 0.9511 - val_loss: 0.2142 - val_accuracy: 0.9336\n",
      "Epoch 60/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9525\n",
      "Epoch 60: val_accuracy improved from 0.93801 to 0.93876, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 85s 97ms/step - loss: 0.1440 - accuracy: 0.9525 - val_loss: 0.2116 - val_accuracy: 0.9388\n",
      "Epoch 61/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1404 - accuracy: 0.9537\n",
      "Epoch 61: val_accuracy did not improve from 0.93876\n",
      "878/878 [==============================] - 82s 93ms/step - loss: 0.1404 - accuracy: 0.9537 - val_loss: 0.2062 - val_accuracy: 0.9372\n",
      "Epoch 62/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1409 - accuracy: 0.9527\n",
      "Epoch 62: val_accuracy did not improve from 0.93876\n",
      "878/878 [==============================] - 81s 92ms/step - loss: 0.1409 - accuracy: 0.9527 - val_loss: 0.2078 - val_accuracy: 0.9374\n",
      "Epoch 63/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9534\n",
      "Epoch 63: val_accuracy did not improve from 0.93876\n",
      "878/878 [==============================] - 83s 95ms/step - loss: 0.1397 - accuracy: 0.9534 - val_loss: 0.2204 - val_accuracy: 0.9338\n",
      "Epoch 64/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9529\n",
      "Epoch 64: val_accuracy did not improve from 0.93876\n",
      "878/878 [==============================] - 83s 94ms/step - loss: 0.1405 - accuracy: 0.9529 - val_loss: 0.2229 - val_accuracy: 0.9350\n",
      "Epoch 65/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1389 - accuracy: 0.9555\n",
      "Epoch 65: val_accuracy did not improve from 0.93876\n",
      "878/878 [==============================] - 87s 99ms/step - loss: 0.1389 - accuracy: 0.9555 - val_loss: 0.2111 - val_accuracy: 0.9378\n",
      "Epoch 66/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.9539\n",
      "Epoch 66: val_accuracy did not improve from 0.93876\n",
      "878/878 [==============================] - 82s 93ms/step - loss: 0.1390 - accuracy: 0.9539 - val_loss: 0.2144 - val_accuracy: 0.9367\n",
      "Epoch 67/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1385 - accuracy: 0.9536\n",
      "Epoch 67: val_accuracy did not improve from 0.93876\n",
      "878/878 [==============================] - 85s 96ms/step - loss: 0.1385 - accuracy: 0.9536 - val_loss: 0.2135 - val_accuracy: 0.9356\n",
      "Epoch 68/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1382 - accuracy: 0.9556\n",
      "Epoch 68: val_accuracy improved from 0.93876 to 0.93908, saving model to best_model2.hdf5\n",
      "878/878 [==============================] - 87s 99ms/step - loss: 0.1382 - accuracy: 0.9556 - val_loss: 0.2096 - val_accuracy: 0.9391\n",
      "Epoch 69/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9548\n",
      "Epoch 69: val_accuracy did not improve from 0.93908\n",
      "878/878 [==============================] - 86s 98ms/step - loss: 0.1379 - accuracy: 0.9548 - val_loss: 0.2115 - val_accuracy: 0.9368\n",
      "Epoch 70/70\n",
      "878/878 [==============================] - ETA: 0s - loss: 0.1384 - accuracy: 0.9543\n",
      "Epoch 70: val_accuracy did not improve from 0.93908\n",
      "878/878 [==============================] - 85s 96ms/step - loss: 0.1384 - accuracy: 0.9543 - val_loss: 0.2299 - val_accuracy: 0.9294\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
    "model2.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\n",
    "model2.add(layers.Dense(3,activation='softmax'))\n",
    "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model2.fit(X_train, y_train, epochs=70,validation_data=(X_test, y_test),callbacks=[checkpoint2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fefa37-0d7e-410c-806b-7cef2b395935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a84e8682-2b95-4df7-858f-68b4abc24689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best model obtained during training\n",
    "\n",
    "best_model = keras.models.load_model(\"best_model2.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e491200-ee7a-4086-a4dc-85497c23e2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293/293 - 6s - loss: 0.2096 - accuracy: 0.9391 - 6s/epoch - 20ms/step\n",
      "Model accuracy:  0.9390830397605896\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Model accuracy: ',test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ae3fed0-6a17-41f7-af0d-ee3e184c8b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "319fdbce-1c4d-491b-a1c4-8a6a6bb81083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(y_test.argmax(axis=1), np.around(predictions, decimals=0).argmax(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42c4dfdb-637f-4cca-abf0-bea188f9a18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyYAAANOCAYAAADkkBgFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABPhUlEQVR4nO3dd5xcVfk/8M/ZQEhIpSqKQuiiIAIC0qWDikqxgeWr0gQL2EARsGChKAiIUvwBgkIAFUSqdFCaIp1QAkivKSSUQPb8/sgSsmSSbDQ7N+X99jWv7Jy55865KzeZZ57nnFNqrQEAAGhSR9MDAAAAEJgAAACNE5gAAACNE5gAAACNE5gAAACNm6+33+Dlu6+07Bc0YMhqn256CDDPGrRA/6aHAPOkp8eMKE2PoSdeeWbkbP35eP5Fl2nk9yhjAgAANE5gAgAANE5gAgAANK7X55gAAABT6JzY9AhmSzImAABA4wQmAABA45RyAQBAO9XOpkcwW5IxAQAAGicwAQAAGqeUCwAA2qlTKVcrMiYAAEDjBCYAAEDjlHIBAEAbVatytSRjAgAANE5gAgAANE4pFwAAtJNVuVqSMQEAABonMAEAABonMAEAABpnjgkAALST5YJbkjEBAAAaJzABAAAap5QLAADaqXNi0yOYLcmYAAAAjROYAAAAjVPKBQAA7WRVrpZkTAAAgMYJTAAAgMYp5QIAgHbqVMrViowJAADQOIEJAADQOKVcAADQRtWqXC3JmAAAAI0TmAAAAI1TygUAAO1kVa6WZEwAAIDGCUwAAIDGCUwAAIDGmWMCAADtZLnglmRMAACAxglMAACAxinlAgCAduqc2PQIZksyJgAAQOMEJgAAQOOUcgEAQDtZlaslGRMAAKBxAhMAAKBxSrkAAKCdOpVytSJjAgAANE5gAgAANE4pFwAAtJNVuVqSMQEAABonMAEAABonMAEAABpnjgkAALST5YJbkjEBAAAaJzABAAAap5QLAADaqNaJTQ9htiRjAgAANE5gAgAANE4pFwAAtJOd31uSMQEAABonMAEAABqnlAsAANrJBostyZgAAACNE5gAAACNU8oFAADtZFWulmRMAACAxglMAACAxinlAgCAduqc2PQIZksyJgAAQOMEJgAAQOMEJgAAQOPMMQEAgHayXHBLMiYAAEDjBCYAAEDjlHIBAEA7dSrlakXGBAAAaJzABAAAaJxSLgAAaCercrUkYwIAADROYAIAADROKRcAALSTVblakjEBAAAaJzABAAAap5QLAADaSSlXSzImAABA4wQmAABA4wQmAABA48wxAQCANqp1YtNDmC3JmAAAAI0TmAAAAI1TygUAAO1kueCWZEwAAIDGCUwAAIDGKeUCAIB2qkq5WpExAQAAGicwAQAAGqeUCwAA2smqXC3JmAAAAI0TmAAAAI1TygUAAO1kVa6WZEwAAIDGCUwAAIDGCUwAAIDGmWMCAADtZLnglmRMAACAxglMAACAxinlAgCAdrJccEsyJgAAQOMEJgAAQOOUcgEAQDtZlaslGRMAAKBxAhMAAKBxSrkAAKCdlHK1JGMCAAA0TmACAAA0TikXAAC0kw0WW5IxAQAAGicwAQAAGjfdUq5SysLTe73W+tysHQ4AAMzlrMrV0ozmmPwzSU1SWrxWkywzy0cEAADMc6YbmNRah7VrIAAAwLyrx6tylVIWSrJ8kn6vtdVar+qNQQEAAPOWHgUmpZQvJvlqkiWT/DvJOkn+kWSTXhsZAADMjSwX3FJPV+X6apL3Jnmo1vr+JO9JMrq3BgUAAMxbehqYvFRrfSlJSikL1FrvTrJi7w0LAACYl/R0jskjpZShSf6c5JJSyqgkD/XWoAAAYK5lueCWehSY1Fo/2vXjQaWUy5MMSXJhr40KAACYp8ywlKuU0qeUcvdrz2utV9Zaz621TujdoTGr3P+fx/LF7/08a+24Zzb93DdzzGnnZOLEGUfq9/3nsex24C+y1o57ZsOd984Pjz0tL7z40uTXJ07szG/PvjCf3e+QbLDz3tlg572z24G/yO33PtiLVwOzr5VWWj4XXPCHPPfciIwceWMOOGCfdHTMuGJ28OBBOe64w/L447flySdvz0knHZmFFx46+fWOjo58/et75NJLz8qjj96SRx+9Jeedd2rWWGPVqc710kv/mepx5ZV/noVXCbOfFVZcNmefe1Ieevzfue3uq/Pt73ylR/feoMED88tjfpx7H7oh9//nphx7/GFZaKGh3Y456lc/ydNjRkz1WG757lu5rbjSchn+pxPz0OP/zt0jr8shPz8oAwYsOCsvE+Z6M8yY1FonllJGlFLeXmv9TzsGxawzdtz47HrAL7LM25bIkd/dMw8//nQO+39nprPWfHnnj0yz3/PjX8gX9z88S731TTnkm7tmzPPj8/OTz84zo0bnyO/smSR5ecKEnHj2hfnIpuvmi9tvnZTk9L9ens/ue0h+97NvZ+XllmrTVULzhg4dkvPP/33uvvve7LjjFzNs2FL52c/2T0dHRw466LDp9j3ttF9l+eWHZY89vpXOzs4cfPB+OfPME7LppjskSfr375dvfONLOeWU4TnkkGNSa80ee3w2l112djbeeLvcfPNt3c73i1/8Jn/60/mTnz///PhZf8EwmxgydHDOPuekjBhxXz7zqS9l6WFvz/d/9O10dHTkJz86Yrp9TzzpiCyz3LDs/eX909nZmQO+/42c8vtj8qGtd+p23D0j7s9XvrRft7aH//PI5J8HDR6YP/7l5Iy878Hs8n97Z+GFh+aAH3wzb3rTYvnsTnvOsmtlLmJVrpZ6OsdkoSR3lFJuSDL5X7ha67a9MipmmeEXXpWXJrySX+y3RwYu2D/vWy0Z9+KL+fUf/pL/227LDFywf8t+Z1xwZV6e8EqO+u5eGTxw0jc+QwYNyFcOPiZ33Ptg3rn80lmgb99ccNzBGTxwwOR+66z6jnzoS9/LH/56eX741c+14Qph9rDLLjunf/9++fjHd83zz49LcnUGDx6Y/fffO4cf/uuutqmtvfbq2XzzjbLZZjvkmmtuSJI89tgTueaav2STTdbPZZddkxdffCnveMf6GT16zOR+l19+bW6//Yrsscdns+uu3+h2zoceeiQ33HBzr10rzE4+9/lPpF//BfK5nffKuOfH58rL/55Bgwbmm/vulaOOPD7jphGYr/ne1fL+TTfItlvvlH/8/aYkyeOPPZmLLz8rG278vlx1xT8mH/vCCy/mnzfdMs0xfP6Ln0r/fv2y0yd2z9gxzydJnntudE4749d593velVtuvn0WXjHMvXq6Ktf3knwwyQ+SHD7Fg9ncNf+8Peu9Z+VuAcjWG7w3L014JTfdfs80+40Y+XBWXm6pyUFJkrxvtZVTSslV/5z07WyfPh3dgpIkmX/++bLs25bI08+NnrUXArO5LbfcOJdccmW3AOTMM8/Nggv2zwYbrDPdfk888dTkoCRJbrrpljzwwH+yxRYbJ0k6Ozu7BSVJ8sorr+TOO+/JEku8adZeCMxhNt1sw1x+6TXdApA/nf3XLLhg/6y73lrT7rf5hnnqyacnByVJcvO/bsuDDz6cTTfbcKbG8K5V3pF///v2yUFJklx5+bXp7OzM5ltsNFPngnlZTwOTbbrmlkx+JNmmNwfGrPHAo09k6SXf3K1ticUWSb8F+uaBR56YZr+XX3kl88/XPaHWp09HOkrJAw8/Ps1+E155JXeN/E+WeqsPS8xbVlhh2dxzz/3d2h5++LGMH/9CVlxx2en0W26qfkly9933Trdf3759s9pqq+Tee0dO9dr++++dceNG5pFH/p3f/ObQLLTQkJm4EpizLLfCMrn3nu73waOPPJ7x41/I8issM41eyfIt+iXJvSPun6rfCisum5EP/zOPPHVbzrvw91l3vfd2e32BBRbIKxNe6db26qsT09nZmRWmcx8zD+vsnL0fDelpYLJ5i7atZ+VA6B3PjxufQS0m3w0euGDGjn9hmv3evsTiuefBh/PKq69ObrvzvocysbMzY8ZNu179+OHnZ8zzL+QT27z/fxs4zGEWWmhIRo8eO1X7qFFjphsYTKvf6NFjMnTotPvtu+9eWXjhITn22JO7tf/ud2dmr732y1ZbfSKHHHJ0tt12q/z1r7/v0URgmBMNHTq4W6biNWNGj83QoYOn2W/I0MEZ06Lf6NFjM2SKfrfdelcO3P9n2fkTu2ePXb6Rjj4dOfPPv817Vl9l8jEPjHwo73zXiplvii/03r3aOzPffPNlqC8GoMem+y9VKWWPUsptSVYqpdw6xeOBJLdNp9+upZSbSik3nTD8L7N6zLTB9lusn1FjxuWnx52eZ0aNyX3/eSwH/+b36dPRkVJa/2dz1U235vizzs/XPrtdhr0hSwPMOltttUm+/e0v57vf/elUGZNddvl6/vznC3LNNTfkl788IZ/97Jez+uqr5AMf2Kyh0cKc7bhfn5KTTvxD/n7tjfnLORdl+20/l8cffypf+/ruk4/53clnZpFFF85PDv1eFl980ay40nI55OcH5tVXX03trA2OHuYsM5r8/vskFyT5SZJ9p2h/vtb63LQ61VqPS3Jckrx895XuyAYNGjgg4154car2seNeyODpLGM4bMklcsCeO+fQE4fnzIuuSkdHyfZbbJiSkkUXmvobqNvvfTDfPPT47LjlRvn0tj4AMe8ZNWpMhgwZNFX7QgsNyahRY1r0eL3fYostPFX70KFDpppXkiRrrLFqTj31mBx//Kk5+ugTZziuiy++Is8/Py7vec+78pe/XDzD42FOM3r02AwaPHCq9iFDB7fMRr5mzOixWWSRVvfe4IyZTr8XX3wpl158ZbbY6vXKgPvuHZmvf/WA/PDH++Vzn/9EJk6cmFNOGp5ak6eefHomr4h5gg0WW5puYFJrHZNkTCnl2294aWApZaDlg2d/w9765qnmkjzx9HN56eUJM8xqfHSz9bPNhmvnoceezCJDB2fooIHZ8NN7Z7vN1+923IOPPpk9f3hU1l51pey7yydm+TXAnOCee+6fak7IkksukQEDFsyIEVPPIXm9331Zb71PTdW+4orL5dxzL+rWttxyw/KnP52Uyy+/Nvvsc+BMja/6ioi51H33jJxqTshb3vrmDBiwYMs5JK+5956RWeeza0zVvtwKy+SC8/423festaa+4ab6/aln5+wz/5Jlll06zzz9bJ59dlTueeD6nHrKmTNxNTBv62nR8V+TnNf156VJRmZSJoXZ3PprvCt/v/mOjH/h9Y0RL7zmpvTrO3/WfNcKM+y/QN/5s8LSS2aRoYNz3pXXpbOzZsv115z8+tPPjc7uBx2Rt715sfzsG19Mnz7q2Jk3XXTRFdlss40ycIqV6nbY4UN54YUXc/XV10233xJLLJ511319Mu3qq6+aZZZZKhdffMXktje/efGcd97vMnLkQ/nMZ/ZKZw+/bdt8840yaNDA/Otf06y+hTnapX+7Ku/fdP0MmOLe+8h22+SFF17M36+9Ydr9Lrkqb3rz4ll7ndeDk3e/510ZNuztufRvV02zX79+C2SzLTfOLbfcMdVrL788IXfdeU+efvrZ7PjxbdPR0ZFz/uTjEvRUeWPE36NOpaye5Eu11i/O6FilXM0aO258PrLXgVnu7W/N57fbMo88+UwO/e2Z2flDm3bbYPEDu303a75rhXz/y59Nkox74cUcf+b5WeOdy6dPR5/ceNuInHLOxTlwz8/kw5uumyR56eUJ+fS3f5rHnno2P9nnCxky6PVUet/558s7lnl7W6+V7oas9ummhzBPGTp0SG6++dLceeeIHH74sRk27O352c++l6OPPrHbBot33HFVrr76uuy++7cmt/3lL7/LcssNy777/mjyBotPP/3s5A0W+/VbIFde+ecstdSS+dznvprnnhs1ue/LL0+Y/AHpC1/4VFZffdVcdtk1efbZ57Laau/Kvvt+OffcMzIbb/zRHgcz/O8GLdB6jyhmvSFDB+fa6/+au+66N0cdcXyWWvpt+eHB++Y3x57SbYPFG26+OH+/9sZ8ba/vTm4b/scTMmzZpXPQ/j/r2mDxm3nm6Wcnb7A4aPDA/P6M3+TM4efmgZEPZZFFFspuX/pcVll15Xxgy09O3p9k4KAB2fsbe+S6a2/MqxMnZv0N1s4ee/1f9vnK93L67//U1t/HvO7pMSNK02PoiReH/2C2/nzc/2MHzPD3WErZKsmRSfokOaHW+tM3vP72JCcnGdp1zL611vPfeJ4p9XSDxW5qrf8qpaz93/SlvQYPHJDjf7BPfnzcH/Llg4/JoAH98+ltN8sen/hQt+MmdnZm4hQfWjo6OnL3yP/k7IuvzssTXslyb39LDvvWbtlknfdMPubZ0WMz4oFJO9/u9cOju53vLYsvkguP/0kvXhnMXkaPHpOtt/5kjjjiBzn77N9m9OixOeqoE/LDH/6i23Hzzdcnffr06da288575tBDD8hvfnNoOjo6csEFl3Yr1XrTmxbLu9/9ziTJn/98Ure+Dz30cFZccb0kyciRD2XnnXfIRz6ydQYPHpgnn3w6p532x3z/+4cJSphrjRk9Nttt+7n89LADcurpv87YMWPz61+dnEN+clS34/r06ZM+b1id7ov/t3d+9OP9cuTRP05HR0cuvujyfOdbB09+fcLLE/LMs89ln2/skUUXWyQvv/Rybrrx3/nwNjt32zSxc2JnVln1Hfn0Z3dMv379cvdd9+YLn/1qLvjrpb178cy55vD62lJKnyTHZNLKvY8kubGUcm6t9c4pDts/yfBa67GllJWTnJ9k6emetycZk1LKPlM87UiyepJFaq1bzqivjAk0Q8YEmiNjAs2YYzImZ3x/tv583P/jB07391hKeV+Sg16LBUop+yVJrfUnUxzzmyQja60/6zr+8FrrutM7b08zJlMuNfNqJs01ObuHfQEAgDlEKWXXJLtO0XRc16q7r3lrkoeneP5IkjdWUx2U5OJSypeTDEgyw2VbexSY1Fq/3zXIBWut096VDwAAmL7ZvLx2yq0//gefTHJSrfXwrozJ70op76q1TvPie7SEUinlfaWUO5Pc3fX83aWUX/2PgwUAAOY8jyZ52xTPl+xqm9IXkgxPklrrP5L0S7Lo9E7a07Vdj0iyZZJnu05+S5INe9gXAACYe9yYZPlSyrBSSt8kn0hy7huO+U+STZOklPKOTApMprvjaI9X5aq1PlxKt3kwE3vaFwAA6DKbl3LNSK311VLKXkkuyqSlgH9ba72jlPKDJDfVWs9N8vUkx5dS9k5Sk3yuzmDVrZ4GJg+XUtZNUksp8yf5apK7/tuLAQAA5lxde5Kc/4a2A6b4+c4k683MOXtayrV7kj0zaQb+o0lW63oOAADwP+vpqlzPJNmpl8cCAABzv2kvTDVPm25gUko5YDov11rrD2fxeAAAgHnQjDIm41u0Dcik5b8WSSIwAQAA/mfTDUxqrYe/9nMpZVAmTXr/vySnJzl8Wv0AAIBpmMNX5eotM5xjUkpZOMk+mTTH5OQkq9daR/X2wAAAgHnHjOaYHJpku0zakn6VWuu4towKAACYp8woY/L1JC8n2T/Jd6fYYLFk0uT3wb04NgAAmPtMf5/BedaM5pj0dJ8TAACA/5rAAwAAaJzABAAAaFyPdn4HAABmEcsFtyRjAgAANE5gAgAANE4pFwAAtJNSrpZkTAAAgMYJTAAAgMYp5QIAgHaqSrlakTEBAAAaJzABAAAap5QLAADaqHbWpocwW5IxAQAAGicwAQAAGqeUCwAA2skGiy3JmAAAAI0TmAAAAI0TmAAAAI0zxwQAANrJzu8tyZgAAACNE5gAAACNU8oFAADtZOf3lmRMAACAxglMAACAxinlAgCAdrLze0syJgAAQOMEJgAAQOOUcgEAQDsp5WpJxgQAAGicwAQAAGicUi4AAGinaoPFVmRMAACAxglMAACAxglMAACAxpljAgAA7WS54JZkTAAAgMYJTAAAgMYp5QIAgHbqtFxwKzImAABA4wQmAABA45RyAQBAO1WrcrUiYwIAADROYAIAADROKRcAALSTVblakjEBAAAaJzABAAAap5QLAADaqHZalasVGRMAAKBxAhMAAKBxSrkAAKCdrMrVkowJAADQOIEJAADQOIEJAADQOHNMAACgnarlgluRMQEAABonMAEAABqnlAsAANrJcsEtyZgAAACNE5gAAACNU8oFAADt1GlVrlZkTAAAgMYJTAAAgMYp5QIAgHayKldLMiYAAEDjBCYAAEDjlHIBAEA7VatytSJjAgAANE5gAgAANE5gAgAANM4cEwAAaCfLBbckYwIAADROYAIAADROKRcAALRR7bRccCsyJgAAQOMEJgAAQOOUcgEAQDtZlaslGRMAAKBxAhMAAKBxSrkAAKCdlHK1JGMCAAA0TmACAAA0TikXAAC0U7XBYisyJgAAQOMEJgAAQOOUcgEAQDtZlaslGRMAAKBxAhMAAKBxAhMAAKBx5pgAAEAbVXNMWpIxAQAAGicwAQAAGqeUCwAA2kkpV0syJgAAQOMEJgAAQOOUcgEAQDt1djY9gtmSjAkAANA4gQkAANA4pVwAANBOVuVqScYEAABonMAEAABonFIuAABoJ6VcLcmYAAAAjROYAAAAjROYAAAAjTPHBAAA2qhWc0xakTEBAAAaJzABAAAap5QLAADayXLBLcmYAAAAjROYAAAAjVPKBQAA7aSUqyUZEwAAoHECEwAAoHFKuQAAoI2qUq6Wej0wGbLap3v7LYAWRl9ycNNDgHnWatv/sukhAMxxlHIBAACNU8oFAADtpJSrJRkTAACgcQITAACgcQITAACgceaYAABAO3U2PYDZk4wJAADQOIEJAADQOKVcAADQRnZ+b03GBAAAaJzABAAAaJxSLgAAaCelXC3JmAAAAI0TmAAAAI1TygUAAO1kg8WWZEwAAIDGCUwAAIDGKeUCAIA2ssFiazImAABA4wQmAABA45RyAQBAO1mVqyUZEwAAoHECEwAAoHECEwAAoHHmmAAAQBtZLrg1GRMAAKBxAhMAAKBxSrkAAKCdLBfckowJAADQOIEJAADQOKVcAADQRlUpV0syJgAAQOMEJgAAQOOUcgEAQDsp5WpJxgQAAGicwAQAAGicUi4AAGgjq3K1JmMCAADMlFLKVqWUEaWU+0op+07jmI+VUu4spdxRSvn9jM4pYwIAAPRYKaVPkmOSbJ7kkSQ3llLOrbXeOcUxyyfZL8l6tdZRpZTFZ3ReGRMAAGBmrJXkvlrryFrrhCSnJ/nwG47ZJckxtdZRSVJrfWpGJ5UxAQCAdprz55i8NcnDUzx/JMnabzhmhSQppVybpE+Sg2qtF07vpAITAABgslLKrkl2naLpuFrrcTN5mvmSLJ9k4yRLJrmqlLJKrXX09DoAAAAkSbqCkOkFIo8medsUz5fsapvSI0mur7W+kuSBUso9mRSo3Ditk5pjAgAAbVQ7Z+9HD9yYZPlSyrBSSt8kn0hy7huO+XMmZUtSSlk0k0q7Rk7vpAITAACgx2qtrybZK8lFSe5KMrzWekcp5QellG27DrsoybOllDuTXJ7km7XWZ6d3XqVcAADATKm1np/k/De0HTDFzzXJPl2PHhGYAABAG9n5vTWlXAAAQOMEJgAAQOOUcgEAQBsp5WpNxgQAAGicwAQAAGicUi4AAGinWpoewWxJxgQAAGicwAQAAGicUi4AAGgjq3K1JmMCAAA0TmACAAA0TmACAAA0zhwTAABoo9ppueBWZEwAAIDGCUwAAIDGKeUCAIA2slxwazImAABA4wQmAABA45RyAQBAG9VqVa5WZEwAAIDGCUwAAIDGKeUCAIA2sipXazImAABA4wQmAABA45RyAQBAG9VOq3K1ImMCAAA0TmACAAA0TmACAAA0zhwTAABoo1qbHsHsScYEAABonMAEAABonFIuAABoI8sFtyZjAgAANE5gAgAANE4pFwAAtJFSrtZkTAAAgMYJTAAAgMYp5QIAgDaywWJrMiYAAEDjBCYAAEDjlHIBAEAbWZWrNRkTAACgcQITAACgcQITAACgceaYAABAG9VqjkkrMiYAAEDjBCYAAEDjlHIBAEAb1c6mRzB7kjEBAAAaJzABAAAap5QLAADaqNOqXC3JmAAAAI0TmAAAAI1TygUAAG1kg8XWZEwAAIDGCUwAAIDGKeUCAIA2qp1KuVqRMQEAABonMAEAABqnlAsAANqo1qZHMHuSMQEAABonMAEAABonMAEAABpnjgkAALSR5YJbkzEBAAAaJzABAAAap5QLAADaqLMq5WpFxgQAAGhcjwOTUspSpZTNun7uX0oZ1HvDAgAA5iU9KuUqpeySZNckCydZNsmSSX6dZNPeGxoAAMx9qlKulnqaMdkzyXpJxiZJrfXeJIv31qAAAIB5S08Dk5drrRNee1JKmS9J7Z0hAQAA85qersp1ZSnlO0n6l1I2T/KlJH/pvWEBAMDcqfp6v6WeZkz2TfJ0ktuS7Jbk/CT799agAACAeUtPMyYfSXJKrfX4XhwLAAAwj+ppYPKhJL8opVyV5IwkF9ZaX+29YQEAwNzJBout9aiUq9b6f0mWS3Jmkk8mub+UckJvDgwAAJh39DRjklrrK6WUCzJpNa7+mVTe9cVeGhcAADAP6VHGpJSydSnlpCT3Jtk+yQlJ3tyL4wIAAOYhPc2YfCaT5pbsVmt9uRfHAwAAczU7v7fWo8Ck1vrJ3h4IAAAw75puKVcp5ZquP58vpYyd4vF8KWVse4bIzFhppeVzwQV/yHPPjcjIkTfmgAP2SUfHjCv2Bg8elOOOOyyPP35bnnzy9px00pFZeOGhk1/v6OjI17++Ry699Kw8+ugtefTRW3LeeadmjTVW7XaeT396h7z00n9aPo4++iez+nJhtnb/Y09nl8NOzdpf+mk2+/oROebPV2RiZ+cM+93x4GPZ7eenZYOvHJYNvnJYdj381Nw68tGpjhs97oX84JS/ZpN9fpG19vhpPrz/sfnL32/tjUuBOc6yKwzL/zvrV7n5watz1a3n58vf3m2G/x7OP/98+eaBX8mp5x6Xfz90de5+6saWx6270Vo5/Nc/yqU3nZO7n7oxe31zl964BJjnTDdjUmtdv+vPQe0ZDv+LoUOH5Pzzf5+77743O+74xQwbtlR+9rP909HRkYMOOmy6fU877VdZfvlh2WOPb6WzszMHH7xfzjzzhGy66Q5Jkv79++Ub3/hSTjlleA455JjUWrPHHp/NZZednY033i4333xbkuSCCy7Lhht+uNu53/ve9+Twww/KRRdd3jsXDrOhseNfzG6Hn5Zl3rJojtjzY3n46VE5fPjfUmvNXh99/zT7PfHcmOx2+GlZaak35+AvTrqXTr7wH9n956flrO/vmrcsMjRJMu7Fl/N/h5ySBRfom30/tWWGDlwwIx97Jq+8OrEdlweztcFDBuX/nXVM7rvngez52a/nbUsvmW8f9LV0lJIjf/rrafbr179fdtjpw7nt5jty84235X0bvrflcRtssm5WWHn5/OPqG7PNR7borctgLmbn99Z6VMpVSvldrfXTM2qjWbvssnP69++Xj3981zz//LgkV2fw4IHZf/+9c/jhv+5qm9raa6+ezTffKJtttkOuueaGJMljjz2Ra675SzbZZP1cdtk1efHFl/KOd6yf0aPHTO53+eXX5vbbr8gee3w2u+76jSTJM888l2eeea7b+T/1qe0yevSYXHTRFb1y3TA7OvPKf+WlV17Nz7+0Ywb2XyDvSzL+xZfz679clc9ttW4G9l+gZb+rbr0v41+akF98accMWrBfkmS1ZZfMRl/7ea659f587P1rJElO+Ou1eeWViTlx/0+nX9/5kyRrrbR0Oy4NZnuf+Oz2WaDfAvny576V8ePGJ1fekIGDBmSvb+yaE47+3aS2Fp4fOy5rr7BpkmSnz+84zcDkkIOOzM8OPCJJsulWG/XKNcC8qEerciV555RPSinzJVlj1g+H/8WWW26cSy65slsAcuaZ52bBBftngw3WmW6/J554anJQkiQ33XRLHnjgP9lii42TJJ2dnd2CkiR55ZVXcued92SJJd40zXN3dHRku+0+kHPOuTATJkz4L68M5jzX3HZf1n3nMt0CkK3WemdemvBqbhrx0DT7vTqxM336dKT/An0nt/VfoG/69OlIzetfsZ1z7S356AarTQ5KgNdtsOn7cs3l13ULQM7/08Xpv2C/rLXu6v/z+auvu6FXzGiOyX6llOeTrDrl/JIkTyY5py0jpMdWWGHZ3HPP/d3aHn74sYwf/0JWXHHZ6fRbbqp+SXL33fdOt1/fvn2z2mqr5N57R07zmE02WS+LL75ohg8/twdXAHOPB554NsPevEi3tiUWGZJ+fefPg088O81+m62xUvr1nT+HD78kz44dn2fHjs+hZ1ySwQv2y+ZrviNJ8sjTo/Lc8+MzqH+/7HnEH7LGbj/Oxl/7eQ494xKlXJBkmeWWzgP3Pdit7fFHn8wL41/MsOWXamZQMIXOWmbrR1NmNMfkJ0l+Ukr5Sa11vzaNif/SQgsNyejRU69JMGrUmCy00JCZ7jd69JgsvfTbp9lv3333ysILD8mxx548zWN23HHbPPnk07n88mtnMHqYuzz/wkuTS7GmNHhAv4x94cVp9lt86KCc8I2d8+WjzsjvL5008XaxIQNz7N6fysKDBiRJnh076VvgX5x1abZaa+X86mufzD0PP5Wj/nR55uvoyN47btoLVwRzjsFDB2fsmKnLl8eOGZshQwY3MCKgJ3q6XPB+pZSFkiyfpN8U7Vf11sCYvW211Sb59re/nG9/+0fTzJjMP//82XbbrXL66X9KZw9WIgKSp0c/n2/++uysvNQSOeizH0ySnH7ZTdnrl6fnlH0/lyUWGTK5jGTZty6WA7uOWfsdwzL+5Zdz4l+vze7bbpj+CyjxAmDO0tOd37+Y5KokFyX5ftefB03n+F1LKTeVUm6aOLH1hGtmvVGjxmTIkKkXUFtooSEZNWpMix7T7zd06JCp5pUkyRprrJpTTz0mxx9/ao4++sRpnnfLLTfOQgsNUcbFPGnQgv0y7sWp96MdO/6lDF6w/zT7nXTRdXl1YmcO2337rPeuZbPeu5bNz7+0Q/qUkpMvui5JJvd/74rdS1LWWmnpTHh1Yh55etQsvBKY84wdPTaDBg+Yqn3wkMEZM8ZuBzSv1jJbP5rS08nvX03y3iQP1Vrfn+Q9SUZP6+Ba63G11jVrrWv26TPwfx8lPXLPPfdPNSdkySWXyIABC2bEiKnnkLze776ssMJyU7WvuOJyU/Vbbrlh+dOfTsrll1+bffY5cLrj+djHts1//vNI/vGPm2biKmDuMOzNi+SBJ57p1vbEc2Py0oRXsvQb5p5M6cHHn8myb1ks88/XZ3Lb/PP1ybJvWWxywPG2xRfK/PP1mXq5ya7npdhRmHnbyPsezDLLLd2t7c1veVMWHNA/D9w77cUngGb1NDB5qdb6UpKUUhaotd6dZMXeGxb/jYsuuiKbbbZRBg58/VuiHXb4UF544cVcffV10+23xBKLZ911X18WcfXVV80yyyyViy++YnLbm9+8eM4773cZOfKhfOYze023PGvBBfvnAx/YPMOH/+V/uyiYQ62/ynL5++0jM/6l17MmF914Z/r1nS9rrjjtybdLLDIk9z36VLdJ7BNeeTX3PfZ03rLopLli88/XJ+usPCw3jniwW9/r73og/frOn7cvvtCsvRiYw1x96T+y3vvXyYABC05u2+Yjm+fFF17KDX//V4MjA6anp4HJI6WUoUn+nOSSUso5SXzlMJs5/vhT8/LLE3LGGcdlk03Wzxe+8Knsv//e+eUvj++2hPAdd1yVX//6kMnPr7/+X7nkkitz4om/yIc/vFU+9KEtctJJR+baa2/IZZddkyTp12+BnHPOyRk6dEh++tOjssoq78haa70na631nrz73e+caiwf+MBmGTBgwQwfbvE25k07brR6+s4/X/Y55qxcd+fInHXlv3LsuVfl05uv3W0J4Q/ud0wOPOn1AH67Dd6Tp8eMy97HnJmrbr03V95yb752zJl5Zsy4bL/h68uc7vbBDXL3f57I9357bv5+x/05+aJ/5LcX/D1f3Ga99J2/R9MHYa51+slnZ8KEV/LLkw7J+zZcKx/79Eez5zd3yUm/Pq3bEsIXXf/H/OgX+3fru8Em62bLD26Sld61QpJkyw9uki0/uEnesuSbJx/zliXfPLl9/r7zZdkVhmXLD26SDTZZtz0XyByv6VW3ZtdVucrMrsVdStkoyZAkF9ZaZ7gxRb9+b7fYdxuttNLyOeKIH2TttdfI6NFjc9JJf8gPf/iLbtmNESOuzVVXXZdddvn65LYhQwbn0EMPyLbbbpmOjo5ccMGl2WefA/Pss5NKR5ZaasmMGPH3lu/50EMPZ8UV1+vWNnz48VlhhWWy2mpWB2rK6EsObnoI87z7H3s6P/n9hbn1/kczaMF++egGq2WPbTdMn47XvxPa+ttHZc0Vl8oPP7/t5Lbr73ogvz73qtz36NNJkuWXXDx7bLth3vuGDRSvvf3+/PKPl+f+x57OwoMWzPYbrp5dPrB+OjqUcjVtte1/2fQQ5nnLrjAs3/vJN7Pamqtk7NhxOevUc3L0ocd1+/fw0pvOyQ1//1f2+8r3u7W99e1vmep8+335+/nTGeclST768Q/mJ0dNXc786H8ey6ZrfrgXroaeuvupG+eIvwCvf8t2s/Xn47Uf+2Mjv8ceBSallIVbND9fa31lRn0FJtAMgQk0R2ACzRCYzBpNBSY9zff/K8nbkoxKUpIMTfJEKeXJJLvUWv/ZO8MDAIC5y2wdlTSop3NMLkmyTa110VrrIkm2TnJeki8l+VVvDQ4AAJg39DQwWafWetFrT2qtFyd5X631uiQLTLsbAADAjPW0lOvxUsq3k5ze9fzjSZ4spfRJYktvAADgf9LTwORTSQ7MpOWCa5Jru9r6JPlYr4wMAADmQk0uyTs761FgUmt9JsmXSykDaq3j3/DyfbN+WAAAwLykR3NMSinrllLuTHJX1/N3l1JMegcAAGaJnpZy/SLJlknOTZJa6y2llA17bVQAADCXqkq5WurpqlyptT78hqaJs3gsAADAPKqnGZOHSynrJqmllPmTfDVdZV0AAAD/q54GJrsnOTLJW5M8muTiJHv21qAAAGBuZa+N1mZmVa6denksAADAPGq6gUkp5YDpvFxrrT+cxeMBAADmQTPKmLxxz5IkGZDkC0kWSSIwAQCAmVBjVa5WphuY1FoPf+3nUsqgTJr0/n9JTk9y+LT6AQAAzIwZzjEppSycZJ9MmmNycpLVa62jentgAADAvGNGc0wOTbJdkuOSrFJrHdeWUQEAwFyqszY9gtnTjDZY/HqStyTZP8ljpZSxXY/nSylje394AADAvGBGc0x6vDM8AADAf0vgAQAANK6nO78DAACzQKflgluSMQEAABonMAEAABqnlAsAANrIzu+tyZgAAACNE5gAAACNU8oFAABt1Nn0AGZTMiYAAEDjBCYAAEDjlHIBAEAbWZWrNRkTAACgcQITAACgcUq5AACgjazK1ZqMCQAA0DiBCQAA0DiBCQAA0DhzTAAAoI3MMWlNxgQAAGicwAQAAGicUi4AAGgjO7+3JmMCAAA0TmACAAA0TikXAAC0UadKrpZkTAAAgMYJTAAAgMYp5QIAgDbqtCpXSzImAABA4wQmAABA45RyAQBAG9WmBzCbkjEBAAAaJzABAAAap5QLAADaqLPpAcymZEwAAIDGCUwAAIDGCUwAAIDGmWMCAABt1Fns/N6KjAkAANA4gQkAANA4pVwAANBGdn5vTcYEAABonMAEAABonFIuAABoIzu/tyZjAgAANE5gAgAANE4pFwAAtFGn/RVbkjEBAAAaJzABAAAaJzABAIA26kyZrR89UUrZqpQyopRyXyll3+kct30ppZZS1pzROQUmAABAj5VS+iQ5JsnWSVZO8slSysotjhuU5KtJru/JeQUmAADAzFgryX211pG11glJTk/y4RbH/TDJz5K81JOTCkwAAIDJSim7llJumuKx6xsOeWuSh6d4/khX25TnWD3J22qtf+3p+1ouGAAA2qg2PYAZqLUel+S4/7Z/KaUjyc+TfG5m+smYAAAAM+PRJG+b4vmSXW2vGZTkXUmuKKU8mGSdJOfOaAK8wAQAAJgZNyZZvpQyrJTSN8knkpz72ou11jG11kVrrUvXWpdOcl2SbWutN03vpEq5AACgjeb0nd9rra+WUvZKclGSPkl+W2u9o5TygyQ31VrPnf4ZWhOYAAAAM6XWen6S89/QdsA0jt24J+dUygUAADROxgQAANqos+kBzKZkTAAAgMYJTAAAgMYp5QIAgDaa3TdYbIqMCQAA0DiBCQAA0DilXAAA0EZz+gaLvUXGBAAAaJzABAAAaJxSLgAAaCMbLLYmYwIAADROYAIAADROYAIAADTOHBMAAGgjc0xakzEBAAAaJzABAAAap5QLAADaqNr5vSUZEwAAoHECEwAAoHFKuQAAoI2sytWajAkAANA4gQkAANA4pVwAANBGSrlakzEBAAAaJzABAAAap5QLAADaqDY9gNmUjAkAANA4gQkAANA4gQkAANA4c0wAAKCNOkvTI5g9yZgAAACNE5gAAACNU8oFAABtZOf31mRMAACAxglMAACAxinlAgCANlLK1ZqMCQAA0DiBCQAA0DilXAAA0Ea16QHMpmRMAACAxglMAACAxinlAgCANuosTY9g9iRjAgAANE5gAgAANE5gAgAANM4cEwAAaCM7v7cmYwIAADROYAIAADROKRcAALSRnd9bkzEBAAAaJzABAAAap5QLAADaqFMxV0syJgAAQON6PWPyaufE3n4LoIUhm3+n6SHAPGvcI1c2PQSAOY5SLgAAaCMbLLamlAsAAGicwAQAAGicUi4AAGgja3K1JmMCAAA0TmACAAA0TikXAAC0kVW5WpMxAQAAGicwAQAAGicwAQAAGmeOCQAAtFFnaXoEsycZEwAAoHECEwAAoHFKuQAAoI067f3ekowJAADQOIEJAADQOKVcAADQRgq5WpMxAQAAGicwAQAAGqeUCwAA2qiz6QHMpmRMAACAxglMAACAxinlAgCANrLBYmsyJgAAQOMEJgAAQOMEJgAAQOPMMQEAgDYyw6Q1GRMAAKBxAhMAAKBxSrkAAKCN7PzemowJAADQOIEJAADQOKVcAADQRnZ+b03GBAAAaJzABAAAaJxSLgAAaCOFXK3JmAAAAI0TmAAAAI1TygUAAG1kg8XWZEwAAIDGCUwAAIDGKeUCAIA2qtblaknGBAAAaJzABAAAaJzABAAAaJw5JgAA0EaWC25NxgQAAGicwAQAAGicUi4AAGijTssFtyRjAgAANE5gAgAANE4pFwAAtJFCrtZkTAAAgMYJTAAAgMYp5QIAgDayKldrMiYAAEDjBCYAAEDjlHIBAEAbdTY9gNmUjAkAANA4gQkAANA4gQkAANA4c0wAAKCNquWCW5IxAQAAGicwAQAAGqeUCwAA2shywa3JmAAAAI0TmAAAAI1TygUAAG1kVa7WZEwAAIDGCUwAAIDGKeUCAIA2sipXazImAABA4wQmAABA45RyAQBAG3VWq3K1ImMCAAA0TmACAAA0TmACAAA0zhwTAABoIzNMWpMxAQAAGicwAQAAGqeUCwAA2qhTMVdLMiYAAEDjBCYAAEDjlHIBAEAbVaVcLcmYAAAAjROYAAAAjVPKBQAAbdTZ9ABmUzImAABA4wQmAABA45RyAQBAG9lgsTUZEwAAoHECEwAAoHFKuQAAoI1ssNiajAkAANA4gQkAANA4gQkAANA4c0wAAKCN7PzemowJAADQOIEJAADQOKVcAADQRrVaLrgVGRMAAKBxAhMAAKBxSrkAAKCNOu383pKMCQAA0DiBCQAA0DilXAAA0EY2WGxNxgQAAJgppZStSikjSin3lVL2bfH6PqWUO0spt5ZSLi2lLDWjcwpMAACAHiul9ElyTJKtk6yc5JOllJXfcNjNSdasta6a5Kwkh8zovAITAABoozqb/68H1kpyX611ZK11QpLTk3y42zXWenmt9YWup9clWXJGJxWYAAAAk5VSdi2l3DTFY9c3HPLWJA9P8fyRrrZp+UKSC2b0via/AwAAk9Vaj0ty3Kw4Vyll5yRrJtloRscKTAAAgJnxaJK3TfF8ya62bkopmyX5bpKNaq0vz+ikAhMAAGijuWDn9xuTLF9KGZZJAcknknxqygNKKe9J8pskW9Van+rJSc0xAQAAeqzW+mqSvZJclOSuJMNrrXeUUn5QStm267BDkwxMcmYp5d+llHNndF4ZEwAAYKbUWs9Pcv4b2g6Y4ufNZvacAhMAAGijWuf4Uq5eoZQLAABonMAEAABonFIuAABoo86mBzCb6lHGpJSyQinl0lLK7V3PVy2l7N+7QwMAAOYVPS3lOj7JfkleSZJa662ZtF4xAADA/6ynpVwL1lpvKKVM2fZqL4wHAADmanXO32CxV/Q0Y/JMKWXZZNJvsZSyQ5LHe21UAADAPKWnGZM9kxyXZKVSyqNJHkiyU6+NCgAAmKf0NDB5qNa6WSllQJKOWuvzvTkoAACYW3Uq5Wqpp6VcD5RSjkuyTpJxvTgeAABgHtTTwGSlJH/LpJKuB0opR5dS1u+9YQEAAPOSHpVy1VpfSDI8yfBSykJJjkxyZZI+vTg2AACY69SqlKuVnmZMUkrZqJTyqyT/TNIvycd6bVT02DvesXwuvvCMjB19X/7z4D9z0IHfSEfHjP9vHTx4UE44/ud5+sk78uzTd+WUk4/KwgsvNNVxH/rQFrn5X3/LuLH359ZbLs+OO2471TErr7xCLvjr7zN29H154rHbcvRRP8mAAQt2O+bVCY+2fIx/fuR/f/Ewm1lppeVz4QV/yKjn7skDI2/KAQd8vcf343HHHZ4nHr8tTz15R0466ZdZeOGhk1/v6OjI17++Ry699Ow89uiteezRW/PX807LGmu8u9t5Vlh+mRx5xI9yy78vy6jn7sndd12Tww87KEOGDJ7VlwqzvfsfeChf+Mq+WXOTj+T92+6Uo48/JRMnTpxhv/tGPpRdvvadrLnJR7L+Nh/PDw49Ki+88GK3Y9613tYtH+/Z+EO9dTkwT+hRxqSU8mCSmzMpa/LNWuv43hwUPTN06JBcdMHpueuue7Pd9v+XZZZZOoceckA6OjpywIGHTLfv6b//dZZffpnsuvs309nZmZ/8+Lv541knZuNNtpt8zHrrvjdnnnF8jv31ydl77wOy9Vab5LTfHZPRo0bnkr9dlWTSB6pLLhqee+8dmU/ttEcWXnih/PQn380SSyye7Xf4wuvnWn/qv6z//KeT8vd/3DiLfhvQrKFDh+SC8/+Qu+++Jzvs+IUsM2yp/Oxn30tHR0cOOujQ6fb9/WnHZvnlh2X3Pb6Vzs6aHx+8X84888Rsuun2SZL+/fvlm9/4Uk455cwcesjRqbVmjz0+l8svOzsbbfzR3HzzbUmSTTfdIO9735o57rjf5bbb78qwYUvloAO/mbXXXiMbbLitb+iYZ4wZ+3y++NXvZNlhb88vf3pAHn708Rx29PHprDVf2fWz0+z3/Ljx+fxX9s3Sb3trDvvBvhk95vn8/Fcn5plnR+WXPz1g8nGn/ebnU/Xd69sH5T2rvLNXrgfmFT1dlWvVWuvYXh0JM223XT+d/v37ZYePfTHPPz8uufTqDB48MAd87+s59LBfTWprYZ2118gWW2yc92+yXa6+5vokyWOPPpF//P2v2XSTDXLpZVcnSb77na/l6quvz977TPrL+Ior/56VV14h+39378mByR67fzb9+/fLhz/6uYwZM+k/keeeG5U//+mkrLH6qvnnv25Nklx/w7+6jWHNNd6dxRZbJKefcc6s/8VAA3bZZef0779APvbxXfP88+NyaSbdj/vvv08OP/zYad6Pa6+9ejbffKNsutkOuea1+/GxJ3LtNX/JJpusn8suuyYvvvhSVnrH+hk9eszkfpddfm1uv/3K7LHH57Lrrl9Pkpwx/Jwc++uTJx9z1VXX5dFHH89fzzst66+/dq6++rpe/A3A7GP4n8/PyxMm5Igf75+BAwYkSca/8EJ+deJp+fxOO0xue6PT/3heXn755Rx9yEEZPGhgkmTokEHZ69vfz+133ZN3vWOFJMm73/WObv1uu2tERo0em60326gXrwrmftOtMSilfKvrx4NLKb9846MN42M6ttry/bn4kiu7feA5Y/g5WXDB/tlow/dNu99W788TTzw1OShJkhtv+ndGjnwoW235/iRJ3759s/HG6+bMs/7Sre8ZZ56bddZZI4MHD0qSvPvd78w//3nr5KAkSS7521Xp7OzMNttsOs0xfOLjH8m4ceNz3nkXz9xFw2xqyy3fn0suuarb/Tj8zHOz4IL9s+EG60y33xNPPDU5KEmSm276dx544KFsucWk+7Gzs7NbUJIkr7zySu668568ZYk3TW577rnRU53/3/++PUmyxBTHwdzumutuyrprrd4tANl6043y0ssv56auDGMrd987Mu9caYXJQUmSvO+9q6eUkqumk+G/4JIr079/v2y8/tqz5gKY63WmztaPpsyo+Pmurj9vyqS5JW980KAVV1wuI0bc163t4Ycfy/jxL2TFFZedqX5Jcvfd92XFFZdLkiy77FLp27fvVMfdfde96dOnT1ZYfpkkSb9+C2TChAndjnn11VfT2dmZlVZafppj2GGHD+Xcv1yUF198afoXCXOIFVdYNiPu+S/uxxWWzYh77p+qfdL9OO1+ffv2zWqrvSv33jv9eVrrrL1GkszwOJibPPDQwxm21Nu6tS3x5sXTv98CGfnQI9PsN2HChMw/f/dikj59+qSjo2Tkg/9p2afWmosuuyqbrL9O+vfr978PHuZh0w1Maq2vfV3+Qq315CkfSV7o/eExPQstNCSjR09dYTdq1JgstNDQafcbOiSjx7ToN3p0FlpoyORjkkx13Kiub21fO+7++x7MqquunPnme/0v8jVWXzXzzTdfFp7GGDZYf+0sueQSGT783GlfHMxhFlpoSMZM434cOr37cVr9Ro/J0K77sJV99/1yFl54aI499qRpHtO/f78cfPB+ufKqf0yehwLzgrHPj8vggVOXaw0eNDBjp1FWmSRvX/ItGXHfyLzy6quT2+4ccW8mTuzMmLGt95b+5y2358mnn1XGBbNAT1fl2q+HbcxjTvjtaVlssUVy5BE/ypvetFhWXnmFHHXUjydnTVr5+Mc/kueeG5WLLr6ivYOFucTWW22Sfb/95Xz3uz/JPdPJhPzmN4dlscUWzW67faONo4M51/bbbpVRo8fkxz8/Ns88+1zuG/lQfnT4MenTp2OaK+ydf8kVGTxoYNbryk5CT9TZ/H9Nme7k91LK1km2SfLWN8wpGZzk1da9klLKrkl2TZLSZ0g6OlpPMuN/M2rUmAwZMmiq9oUWGpJRo0ZPu9/oMVls0UWm7jd0aEaNGjP5mCQZMnjQG44ZMvm9k2TEiPuz+x7fyuGHHZTddv10Jk6cmONPOC211jzx5NNTvUefPn2y3Ue3yR//dH5eeeWVnl0ozAFGjRqTwdO4H0dP734cNSaLLtbqfhwy1bySJFljjXfn1FN/leOPPzVHHX3iNM/744O/kw9vu2W2+cBOeeCB1iUoMLcaPGhgnh8/dWHH2OfHdZs/8kbLLPW2HPitr+SQXx6XM885Px0dHdlh262SlCzaYkn9V1+dmL9dcW0233j9zD///LPyEmCeNKNVuR7LpPkl26b7nJLnk+w9rU611uOSHJck8/V9q/Upe8mIEa/PCXnNkku+JQMGLJgRI6auWZ+y3/rrrTVV+4orLptzz70oSXL//Q9lwoQJWXHF5XLVFCv5rLjScpk4cWK3b2lPOvmM/OH0P2f55YflqaeeyTPPPJennrg9v/3tH6Z6j003WT+LL75ozrAaF3OZEffc3+J+XGLG9+M992e9lvfjcpPvx9csv9yw/PlPJ+Xyy6+dvFpeK1/58hez9967ZedP75lrr71hJq8E5nzDlnpbHnjo4W5tjz/5dF586eUss9SS0+273Qe3zAc2f38eeuTRLLzQ0Cw0ZHDW3+bj2f5DW0517PX//HeeGz0m22yujAtmhRnNMbmlaz7Jsm+YY/LHWuuoNo2RabjwosuzxeYbZeAUdbQf2/FDeeGFF3PlVf+Ydr8LL88SS7wp66373slta6y+apZddulceNHlSSZNALziir9nh+0/2K3vx3bYNtdd98+MfUOt7csvv5zbb787Tz31THbaaft0dHRMtaJXMqmM67HHnsgVV/79v7pmmF1ddNHl2Xyz7vfjjjtsmxdeeLFbcN+q3xJLvCnrTnE/rr76qllmmaVy0cWXT25785sXz3nnnZqRIx/Kpz+z5zRLJT/xiY/kZz/7Xr71rR/k7LPPmwVXBnOe9ddZM9de/8+MnyJrcuGlV6bfAgtkzfesMsP+CyzQNyssOyyLLrxQzrvosnR2dmarTTec6rjzL7kiiy2ycN77nlVn6fiZ+3XWOls/mjKjUq7htdaPJbm5lDLlKEuSWmt1JzboN8f9Lnvt+fmcNfyEHHrYrzJs2NtzwPe+niOOPK7bkqV333lNrrr6uuzaVWd+3fX/zMUXX5H/99sj8619fzh5g8Vrrrl+8h4mSXLwj4/IpX87K4cf9v2ce+6F2XrrTbL11pvkAx/cafIxgwYNzHf2+0quvvr6vPrqq9l443Wz99d2y267f2uqcrK+ffvmw9tumZNPOdNGb8x1jj/+1Oz5pf/L8DOOy2GHH5thw96e/fffO0f+8vhu9+Odd1ydq66+Lrvv/s0kyfXX/yuXXHJlfnviL/LtfX+U2llz8MH75Zprb8hll12TJOnXr1/OPeeUDB06JF/72veyyiqv76Hw8ssTcsstdyRJNthgnRx/3OH529+uyvU3/CtrrfWeycc9+ujjefTRJ9rxq4DGfewj2+S0s87JV7/zo3xh5x3zyGOP51e/PS2f+cRHuy8h/LHPZ833rJIf7jepCGTc+PE57uTTs8Zqq2S+Pn1yw79uycl/+GMO+vZXpyptnjBhQi67+h/58NabTXP+CTBzZlTK9dWuPz843aNoxOjRY7LFVh/PL484OH/+0//L6NFjc+Qvj8/3f3B4t+Pmm2++9OnTp1vbJ3faI4cfdlBOOO7wdHR05K/n/y1f2/t73Y659u835mOf2DU/+P63svtun84DDz6cnT+z5+TNFZNk4sSJWe3d78oXPv+p9O/fL7ffMSIf/+RuU5WgJJP2Txk6dEiGD1fGxdxn9Ogx2WrrT+aII36YP579/zJ69Jj88qgT8sMfdt8hus98faa6H3fa+Us59NADc9xvDktHR0fOv+DS7DNFqdab3rRo3v3uSTtK//nPJ3fr++BDD2fFFddNkmy00fvSt2/fbLHFxtlii427HffDH/08P/rRL2bV5cJsbcjgQTnxyJ/k4J8fm72+dVAGDRqQz3zso/nSF3bqdtzEiRPTOfH17GNHR5/cdc/9OevcC/PyyxOy3DJL5fAffSebbrjuVO9x9XU35flx463GBbNQ6ck316WUAUlerLV2llJWSLJSkgtqrTOcvWyOCTSjj2/woDHjHrmy6SHAPGn+RZcpTY+hJzZ466az9efjqx+9tJHfY08/uVyVpF8p5a1JLk7y6SQn9dagAACAeUtPA5NSa30hyXZJflVr3THJO3tvWAAAwLxkRnNMXlNKKe9LslOSL3S19ZnO8QAAQAudDW5iODvracbka5m00/ufaq13lFKWSXL59LsAAAD0TI8yJrXWK5NcWUoZWEoZWGsdmeQrvTs0AABgXtGjjEkpZZVSys1J7khyZynln6UUc0wAAIBZoqdzTH6TZJ9a6+VJUkrZOMnxSaZe2BsAAJgmc0xa6+kckwGvBSVJUmu9IsmAaR8OAADQcz3NmIwspXwvye+6nu+cZGTvDAkAAJjX9DQw+XyS7yf5Y5Ka5OquNgAAYCbUqpSrlekGJqWUfkl2T7JcktuSfL3W+ko7BgYAAMw7ZjTH5OQka2ZSULJ1kkN7fUQAAMA8Z0alXCvXWldJklLKiUlu6P0hAQDA3MuqXK3NKGMyuWyr1vpqL48FAACYR80oY/LuUsrYrp9Lkv5dz0uSWmsd3KujAwAA5gnTDUxqrX3aNRAAAJgXVKVcLfV0g0UAAIBeIzABAAAa19MNFgEAgFnABoutyZgAAACNE5gAAACNE5gAAACNM8cEAADayM7vrcmYAAAAjROYAAAAjVPKBQAAbWS54NZkTAAAgMYJTAAAgMYp5QIAgDayKldrMiYAAEDjBCYAAEDjlHIBAEAbVaVcLcmYAAAAjROYAAAAjVPKBQAAbdRpg8WWZEwAAIDGCUwAAIDGKeUCAIA2sipXazImAABA4wQmAABA4wQmAABA48wxAQCANrJccGsyJgAAQOMEJgAAQOOUcgEAQBtZLrg1GRMAAKBxAhMAAKBxSrkAAKCNrMrVmowJAADQOIEJAADQOKVcAADQRlblak3GBAAAaJzABAAAaJxSLgAAaCOrcrUmYwIAADROYAIAADROYAIAADTOHBMAAGgjywW3JmMCAAA0TmACAAA0TikXAAC0Ua2dTQ9htiRjAgAANE5gAgAANE4pFwAAtFGnVblakjEBAAAaJzABAAAap5QLAADaqFalXK3ImAAAAI0TmAAAAI1TygUAAG1kVa7WZEwAAIDGCUwAAIDGKeUCAIA2sipXazImAABA4wQmAABA4wQmAABA48wxAQCANuo0x6QlGRMAAKBxAhMAAKBxSrkAAKCNqp3fW5IxAQAAGicwAQAAGqeUCwAA2sjO763JmAAAAI0TmAAAAI1TygUAAG3UaVWulmRMAACAxglMAACAxinlAgCANrIqV2syJgAAQOMEJgAAQOMEJgAAQOPMMQEAgDbqNMekJRkTAACgcQITAACgcUq5AACgjSwX3JqMCQAA0DiBCQAA0DilXAAA0EadUcrViowJAADQOIEJAADQOKVcAADQRlblak3GBAAAaJzABAAAaJxSLgAAaKNOpVwtyZgAAACNE5gAAACNE5gAAACNM8cEAADaqNr5vSUZEwAAoHECEwAAoHFKuQAAoI0sF9yajAkAANA4gQkAANA4pVwAANBGVSlXSzImAABA4wQmAABA45RyAQBAG9lgsTUZEwAAoHECEwAAoHFKuQAAoI2sytWajAkAANA4gQkAANA4pVwAANBGSrlakzEBAAAaJzABAAAaJzABAAAaZ44JAAC0kRkmrcmYAAAAjROYAAAAjSuWK2N6Sim71lqPa3ocMK9x70Ez3HvQHBkTZmTXpgcA8yj3HjTDvQcNEZgAAACNE5gAAACNE5gwI+psoRnuPWiGew8aYvI7AADQOBkTAACgcQITAACgcQKTuVQppZZSDp/i+TdKKQf9l+caWkr50n/Z98FSyqL/TV+YU8zK+20G7/OdNzz/+6x+D5hTlVImllL+XUq5vZRyZillwZns/5ZSylldP69WStlmite2LaXsO6vHDHQnMJl7vZxku1kUFAxN0jIwKaXMNwvOD3O6WXm/TU+3wKTWum4vvx/MSV6sta5Wa31XkglJdp+ZzrXWx2qtO3Q9XS3JNlO8dm6t9aezbKRASwKTudermbSyyN5vfKGUslgp5exSyo1dj/W62g8qpXxjiuNuL6UsneSnSZbt+ibq0FLKxqWUq0sp5ya5s+vYP5dS/llKuaOUYnMq5jX/zf22WCnlkq575oRSykOvBTat7qdSyk+T9O+6D0/rahvX9efppZQPTPGeJ5VSdiil9Om6Z28spdxaStmt138TMHu4OslypZSFu+6nW0sp15VSVk2SUspGXffSv0spN5dSBpVSlu76d69vkh8k+XjX6x8vpXyulHJ0KWVI173a0XWeAaWUh0sp85dSli2lXNh1715dSlmpweuHOZLAZO52TJKdSilD3tB+ZJJf1Frfm2T7JCfM4Dz7Jrm/65uob3a1rZ7kq7XWFbqef77WukaSNZN8pZSyyKy5BJhjzOz9dmCSy2qt70xyVpK3T9Fnqvup1rpvXv9GeKc3vMcZST6WJF0fqjZN8tckX0gypuu935tkl1LKsFl0vTBb6srkb53ktiTfT3JzrXXVTMo4ntJ12DeS7FlrXS3JBklefK1/rXVCkgOSnNF1v50xxWtjkvw7yUZdTR9MclGt9ZVM+nLiy1337jeS/Kq3rhHmVspw5mK11rGllFOSfCVT/KWbZLMkK5dSXns+uJQycCZPf0Ot9YEpnn+llPLRrp/flmT5JM/+F8OGOdJ/cb+tn+SjXX0vLKWMmqLPzN5PFyQ5spSyQJKtklxVa32xlLJFklVLKa+VpwzpOtcD0zgPzMn6l1L+3fXz1UlOTHJ9Jn0hkFrrZaWURUopg5Ncm+TnXdnHP9ZaH5niHp2RM5J8PMnlST6R5Fdd9/S6Sc6c4jwL/O+XBPMWgcnc74gk/0ry/6Zo60iyTq31pSkPLKW8mu5ZtH7TOe/4KfptnEkfvt5Xa32hlHLFDPrC3OqI9Px+a3mC/+Z+qrW+1HXclpn0gen0106XSd/gXjRzlwFzpBe7MiCTTes+q7X+tJTy10yaR3JtKWXLJC+1PHhq5yb5cSll4SRrJLksyYAko9/4/sDMUco1l6u1PpdkeCaVdLzm4iRffu1JKWW1rh8fzKQSrZRSVk/yWsnH80kGTedthiQZ1fUhaqUk68yKscOcZibvt2vzevnVFkkW6mqf3v30Sill/mm8/RlJ/i+TylIu7Gq7KMker/UppaxQShnw310dzJGuTrJTMjnof6Yru7lsrfW2WuvPktyY5I3zQab5716tdVxXnyOTnFdrnVhrHZvkgVLKjl3vVUop7+6NC4K5mcBk3nB4kilXC/pKkjW7JgPemddXLjk7ycKllDuS7JXkniSptT6bSd8o3V5KObTF+S9MMl8p5a5Mmih/XS9dB8wJenq/fT/JFqWU25PsmOSJTPowNL376bgkt742+f0NLs6kuve/ddXIJ5Pms9yZ5F9d7/ObyJQzbzkoyRqllFsz6X76bFf717r+Tbs1ySuZVA45pcszqQTz36WUj7c47xlJdu768zU7JflCKeWWJHck+fCsuwyYN5Raa9NjAJjndM0HmVhrfbWU8r4kxyoDAWBe5pszgGa8PcnwrmVHJyTZpeHxAECjZEwAAIDGmWMCAAA0TmACAAA0TmACAAA0TmACAAA0TmACAAA07v8DXVfmBrBzKkIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "conf_matrix = pd.DataFrame(matrix, index = ['Neutral','Negative','Positive'],columns = ['Neutral','Negative','Positive'])\n",
    "#Normalizing\n",
    "conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (15,15))\n",
    "sns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b687f99f-03f7-4a19-b6e5-453021cfa603",
   "metadata": {},
   "source": [
    "Let's see how it performs on some test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "408eaa8f-50e0-4baa-954a-f613fba67823",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = ['Neutral','Negative','Positive']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df77a376-5eab-46c9-9310-ae7ce176d98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['this experience has been the worst , want a refund'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90903c5d-c40f-4045-b5e7-a0cee1ffe662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['this data science article was preety much good'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98411f61-e0e0-45f5-bef0-5a8c78599f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['I am confused about this experience'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2521069-d8e6-4696-a0b6-276b8d5936d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
